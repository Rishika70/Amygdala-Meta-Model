# -*- coding: utf-8 -*-
"""Amygdala-Meta-Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16keBwoTQRIVVVgYXKipDoNPXYDEEHaFT

# What is Amygdala

**'''
The amygdala is a complex structure located deep within the brain's medial temporal lobe.
It plays a crucial role in processing emotions, particularly fear and anxiety.
It receives sensory input and evaluates its significance, triggering physiological and behavioral responses.
The amygdala's activity is linked to emotional learning, memory formation, and social behavior.
'''**

# Purpose of Project

**The Amygdala Project aims to explore and understand the intricate relationships between ego, attention, and shame through a machine learning model inspired by neuroscience principles. By leveraging insights from brain function—particularly the roles of the prefrontal cortex, limbic system, and attention-regulating regions—this project seeks to foster self-awareness, emotional regulation, and psychological healing in individuals.**

**Incorporating elements of psychological healing, the project integrates gamified, neuroscience-backed interventions designed to guide individuals through self-reflection and emotional growth. These interventions offer tools to address trauma, alleviate shame, and reframe ego-driven thought patterns, fostering a deeper connection to oneself and others**

# Key Objectives:

Enhance Self-Awareness: The model will help users develop a deeper understanding of their emotional responses and behavioral patterns related to ego, attention-seeking, and feelings of shame.

Promote Mindfulness: By implementing the concept of stillness, the project encourages reflective thinking and mindfulness in decision-making processes, allowing users to approach their experiences with calmness and clarity.

Facilitate Neuroplasticity: The project aims to integrate mechanisms that simulate neuroplasticity, enabling the model to adapt and learn from user interactions, thereby promoting personal growth and emotional resilience.

Avoid Stagnation: By emphasizing continuous learning and adaptation, the project seeks to prevent stagnation in emotional growth, ensuring that users remain engaged and proactive in their self-development journeys.

Create a Meta Model for Self-Improvement: The ultimate goal is to create a self-aware machine learning model that not only predicts and responds to user emotions but also evolves over time, becoming more attuned to individual needs and fostering a sense of connection and empathy.

# Import Necessary Libraries
"""

!pip install tensorflow
!pip install langchain
!pip install langchain_text_splitters
!pip install langchain_community
!pip install vectara
!pip install sentence_transformers
!pip install transformers
!pip install accelerate
!pip install bitsandbytes

# Huggingface libraries to run LLM.
!pip install -q -U transformers
!pip install -q -U accelerate
!pip install -q -U bitsandbytes

#LangChain related libraries
!pip install -q -U langchain

#Open-source pure-python PDF library capable of splitting, merging, cropping,
#and transforming the pages of PDF files
!pip install -q -U pypdf

#Python framework for state-of-the-art sentence, text and image embeddings.
!pip install -q -U sentence-transformers

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
import matplotlib.pyplot as plt

from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.embeddings.fake import FakeEmbeddings
from langchain_community.vectorstores import Vectara

"""# Importing Vector Database Vectara"""

import os
import getpass

os.environ["VECTARA_CUSTOMER_ID"] = getpass.getpass("237972845")
os.environ["VECTARA_CORPUS_ID"] = getpass.getpass("3")
os.environ["VECTARA_API_KEY"] = getpass.getpass("zwt_Di8tbWth85d1CzFG_UwiUn48NhhEub1-ujaw9g")

vectorstore = Vectara(
                vectara_customer_id="237972845",
                vectara_corpus_id=3,
                vectara_api_key="zwt_Di8tbWth85d1CzFG_UwiUn48NhhEub1-ujaw9g"
            )

!pip install tensorflow transformers requests

"""# Create Vector Embeddings"""

#Create vectara-embeddings to train ANN

from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings

# Example text data
texts = [
    "The amygdala is a complex structure located deep within the brain's medial temporal lobe.",
    "It plays a crucial role in processing emotions, particularly fear and anxiety.",
    "The amygdala's activity is linked to emotional learning, memory formation, and social behavior."
]

# Initialize SentenceTransformerEmbeddings
embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

# Create embeddings for the text data
vector_embeddings = embeddings.embed_documents(texts)

"""# Data Pre-Processing"""

# Assuming vectara_embeddings is a precomputed tensor from Vectara (pdf embeddings)
def process_vectara_data(vectara_embeddings):
    # Ensure data is in the right shape for the model
    return vectara_embeddings.view(-1, vectara_embeddings.shape[-1])

"""# Implementing Artificial Neural Network


##Step 1: Introduction to Neuroscience and Neuroplasticity
Define Key Terms:

Neuroscience: The study of the nervous system, including the brain and its role in behavior and cognitive functions.
Neuroplasticity: The ability of the brain to reorganize itself by forming new neural connections throughout life, which is essential for learning and recovery from injury.
Importance of Neuroplasticity:

Emphasize how neuroplasticity underlies learning, memory, and the ability to adapt to new experiences or environments.
Discuss its implications for understanding consciousness, particularly how conscious experiences can shape neural pathways.

##Step 2: Explain the Neuroscience Framework
Neural Networks:

Describe how the brain is composed of networks of neurons that communicate through synapses. Each connection can be strengthened or weakened based on experience.
Introduce the concept of artificial neural networks (ANNs) as a simplified model of biological neural networks.
Mathematical Representation of Neural Activity:

Use differential equations to model the dynamics of neural firing. For example, the simple leaky integrate-and-fire model:
𝑑
𝑉
𝑑
𝑡
=
−
𝑉
𝜏
+
𝐼
(
𝑡
)
dt
dV
​
 =−
τ
V
​
 +I(t)
Where:
𝑉
V = membrane potential
𝜏
τ = time constant
𝐼
(
𝑡
)
I(t) = input current to the neuron

##Step 3: Introduce Neuroplasticity in Mathematical Terms
Hebbian Learning Rule:

Introduce the concept of Hebbian plasticity, where "cells that fire together, wire together." The mathematical formulation can be expressed as:
Δ
𝑤
𝑖
𝑗
=
𝜂
⋅
𝑎
𝑖
⋅
𝑎
𝑗
Δw
ij
​
 =η⋅a
i
​
 ⋅a
j
​

Where:
Δ
𝑤
𝑖
𝑗
Δw
ij
​
  = change in weight between neuron
𝑖
i and neuron
𝑗
j
𝜂
η = learning rate
𝑎
𝑖
a
i
​
  and
𝑎
𝑗
a
j
​
  = activations of the pre- and post-synaptic neurons
Stability and Plasticity:

Discuss models that account for stability in neural connections while allowing for plastic changes, such as the Bienenstock-Cooper-Munro (BCM) theory:
𝑤
𝑖
𝑗
(
𝑡
+
1
)
=
𝑤
𝑖
𝑗
(
𝑡
)
+
𝜂
⋅
(
𝑎
𝑖
(
𝑡
)
−
𝜃
)
⋅
𝑎
𝑗
(
𝑡
)
w
ij
​
 (t+1)=w
ij
​
 (t)+η⋅(a
i
​
 (t)−θ)⋅a
j
​
 (t)
Where:
𝜃
θ = threshold level for activity

##Define Neural Dynamics:
"""

def leaky_integrate_and_fire(V0, I, tau, dt, T):
    V = V0
    times = np.arange(0, T, dt)
    voltages = []

    for t in times:
        dV = (-V / tau + I) * dt
        V += dV
        if V >= threshold:
            V = 0  # reset after firing
        voltages.append(V)

    return times, voltages

"""## Simulate Neural Activity:"""

# Parameters
V0 = -70  # initial membrane potential
I = 5  # input current
tau = 20  # time constant
dt = 0.1  # time step
T = 100  # total time
threshold = -50  # firing threshold

times, voltages = leaky_integrate_and_fire(V0, I, tau, dt, T)

# Plotting the results
plt.plot(times, voltages)
plt.title('Leaky Integrate-and-Fire Model')
plt.xlabel('Time (ms)')
plt.ylabel('Membrane Potential (mV)')
plt.grid()
plt.show()

"""Okay, let's analyze the graph generated by the provided code.


**Graph Description:**


The graph visualizes the behavior of a "Leaky Integrate-and-Fire" model, a simplified representation of a neuron's electrical activity.


**X-axis:** Represents time (in milliseconds).


**Y-axis:** Represents the neuron's membrane potential (in millivolts).


**Plot Line:** The line shows how the membrane potential changes over time. It demonstrates how a constant input current (I) causes the membrane potential to gradually rise. When it reaches a threshold (-50 mV in this case), the neuron "fires," and its potential is reset to 0 mV. The leaking behavior of the model is shown as a decline of the membrane potential when the input is removed.


**Interpretation in the Context of Neuroscience:**


* **Membrane Potential:** The neuron's membrane potential is like a voltage across its cell wall.
* **Input Current:** The input current (I) signifies external stimuli or signals from other neurons.
* **Threshold:** The threshold (-50 mV) represents the point at which the neuron becomes sufficiently excited to generate a spike or action potential.
* **Leaking Behavior:** It represents the natural decay of the membrane potential over time, representing how the neuron's charge dissipates without continuous input.


**Relevance to the Amygdala Project:**


The code demonstrates basic principles of how neural activity can be mathematically modeled. This foundation is crucial for the Amygdala Project, where the goal is to develop a machine learning model inspired by neuroscience. The project aims to understand complex brain functions related to emotions (like fear and anxiety) and how they interact with ego, attention, and shame.


By simulating neuronal activity, the project can leverage these foundational principles to potentially build a more sophisticated model capable of adapting to user interaction and simulating neuroplasticity, which is the basis for learning and the ability of the brain to change over time.


**In Essence:** The graph illustrates fundamental elements of how a neuron operates. It is a preliminary step towards building a system capable of replicating and learning from more complex neurological systems like the amygdala.

# Explore Neuroplasticity Simulation
##Hebbian Learning Simulation:

Implement Hebbian learning to see how weights change over time based on neural activity.
"""

def hebbian_learning(weights, a_pre, a_post, learning_rate):
    return weights + learning_rate * a_pre * a_post

"""## Run a simulation

"""

# Initialize weights
weights = np.array([0.1, 0.2, 0.3])
a_pre = np.random.rand(len(weights))  # pre-synaptic activity
a_post = np.random.rand(len(weights))  # post-synaptic activity
learning_rate = 0.01

# Apply Hebbian learning
new_weights = hebbian_learning(weights, a_pre, a_post, learning_rate)
print("Updated weights:", new_weights)

"""The output "Updated weights: [0.10013435 0.20010984 0.30057867]" indicates the result of a Hebbian learning simulation within the provided code. Let's break down what this means:

**Hebbian Learning:** This is a biological learning rule that states "neurons that fire together wire together." Essentially, if two neurons are activated simultaneously, the connection (synaptic weight) between them strengthens.

**Simulation:**
1. **Initial Weights:** The code starts with an array of initial weights: `[0.1, 0.2, 0.3]`. These weights represent the strength of connections between neurons in the artificial neural network.
2. **Pre- and Post-synaptic Activity:**  Random values are generated for `a_pre` and `a_post`, representing the activity of neurons before and after the synapse (connection).
3. **Learning Rate:** A small value (`0.01`) is set for the learning rate, which controls how much the weights are updated in each step.
4. **Hebbian Learning Function:** The code then applies the Hebbian learning rule. The function `hebbian_learning` adjusts the weights based on the pre- and post-synaptic activity and the learning rate.
5. **Updated Weights:** The resulting `new_weights` are printed: `[0.10013435 0.20010984 0.30057867]`.

**Interpretation:** The slight increase in each weight value demonstrates that the Hebbian learning rule has strengthened the connections between neurons based on the simulated neural activity. The specific magnitude of the change depends on the activity levels of the neurons and the learning rate.

**In the context of the Amygdala Project:**  This simulation is a rudimentary example of how the project aims to implement neuroplasticity – the ability of the brain to change its structure and function in response to experiences. By simulating Hebbian learning and the adaptation of synaptic weights, the project seeks to develop a model that can learn and adapt to user interactions over time, reflecting the way the human brain adapts and learns new things.

## Define Ego, Shame, and Attention (Amydgala Framework)
# Ego:

Definition: The ego is often understood as the self-concept, a construct that includes our identity, self-esteem, and self-importance. In psychology, it's seen as a mediator between the conscious mind and the unconscious desires.
Neural Correlates: The prefrontal cortex plays a crucial role in self-referential processing, decision-making, and regulation of social behavior.
Shame:

Definition: Shame is a complex emotion that arises from the perception of failure, inadequacy, or social disapproval. It often leads to feelings of worthlessness and withdrawal.
Neural Correlates: The limbic system, particularly the amygdala and anterior cingulate cortex, is involved in processing emotions related to shame and social rejection.
Attention:

Definition: Attention is the cognitive process of selectively concentrating on certain information while ignoring other stimuli. It is essential for learning and conscious awareness.
Neural Correlates: The parietal lobes and networks involving the thalamus are crucial for attentional control and sensory integration.

##Step 2: Mathematical Framework
# 1. Ego Dynamics
To represent ego in a mathematical framework, we can define a simple model where the ego is influenced by both internal (self-related) and external (social feedback) factors:

Ego Model:
𝑑
𝐸
𝑑
𝑡
=
𝛼
(
𝑆
−
𝐸
)
+
𝛽
(
𝐹
−
𝐸
)
dt
dE
​
 =α(S−E)+β(F−E)
Where:
𝐸
E = ego strength
𝑆
S = self-esteem factor (internal)
𝐹
F = feedback from social interactions (external)
𝛼
α and
𝛽
β are positive constants representing sensitivity to self and social feedback.
# 2. Shame Dynamics
Shame can be modeled similarly, where it is influenced by social feedback and the state of the ego:

Shame Model:
𝑑
𝐻
𝑑
𝑡
=
𝛾
(
𝐸
−
𝐻
)
+
𝛿
(
𝑅
−
𝐻
)
dt
dH
​
 =γ(E−H)+δ(R−H)
Where:
𝐻
H = shame level
𝑅
R = perceived rejection or failure from social feedback
𝛾
γ and
𝛿
δ are sensitivity parameters.
# 3. Attention Dynamics
Attention can be represented through a focus model, where the level of attention shifts based on perceived relevance and task load:

Attention Model:
𝑑
𝐴
𝑑
𝑡
=
𝜖
(
𝑇
−
𝐴
)
−
𝜁
(
𝐻
)
dt
dA
​
 =ϵ(T−A)−ζ(H)
Where:
𝐴
A = attention level
𝑇
T = task demand or relevance
𝐻
H = level of shame (which can detract from attention)
𝜖
ϵ and
𝜁
ζ are constants representing sensitivity to task demand and shame, respectively.
"""

import numpy as np
import matplotlib.pyplot as plt

# Parameters
alpha = 0.1  # sensitivity of ego to self-esteem
beta = 0.05  # sensitivity of ego to feedback
gamma = 0.1  # sensitivity of shame to ego
delta = 0.1  # sensitivity of shame to rejection
epsilon = 0.2  # sensitivity of attention to task
zeta = 0.1    # sensitivity of attention to shame

# Initial conditions
E0 = 50  # initial ego strength
H0 = 30  # initial shame level
A0 = 70  # initial attention level

# Simulation parameters
dt = 0.1  # time step
T = 100   # total time
time = np.arange(0, T, dt)

# Arrays to store values
E = np.zeros(len(time))
H = np.zeros(len(time))
A = np.zeros(len(time))

# Set initial values
E[0] = E0
H[0] = H0
A[0] = A0

# Feedback and task demand functions (example functions)
feedback = np.sin(0.1 * time) * 20 + 50  # Simulated social feedback
task_demand = np.abs(np.sin(0.05 * time) * 50)  # Simulated task demand

# Simulation loop
for i in range(1, len(time)):
    # Update ego, shame, and attention using the models
    E[i] = E[i - 1] + dt * (alpha * (feedback[i] - E[i - 1]) + beta * (feedback[i] - E[i - 1]))
    H[i] = H[i - 1] + dt * (gamma * (E[i - 1] - H[i - 1]) + delta * (task_demand[i] - H[i - 1]))
    A[i] = A[i - 1] + dt * (epsilon * (task_demand[i] - A[i - 1]) - zeta * H[i - 1])

# Plotting the results
plt.figure(figsize=(12, 8))
plt.subplot(3, 1, 1)
plt.plot(time, E, label='Ego Strength', color='blue')
plt.title('Ego Dynamics')
plt.ylabel('Ego')
plt.grid()

plt.subplot(3, 1, 2)
plt.plot(time, H, label='Shame Level', color='red')
plt.title('Shame Dynamics')
plt.ylabel('Shame')
plt.grid()

plt.subplot(3, 1, 3)
plt.plot(time, A, label='Attention Level', color='green')
plt.title('Attention Dynamics')
plt.ylabel('Attention')
plt.xlabel('Time')
plt.grid()

plt.tight_layout()
plt.show()

"""Okay, let's analyze the graphs generated by the provided code.


## **Graph 1: Ego Dynamics**


 * **X-axis:** Represents time.
 * **Y-axis:** Represents the level of Ego Strength.
 * **Plot Line:** Shows how the ego strength fluctuates over time, influenced by the simulated social feedback.


## **Interpretation:**


 The graph likely depicts how the individual's sense of self (Ego) changes based on external factors (social feedback). The fluctuations might reflect responses to positive or negative social interactions, indicating how the individual's self-esteem and self-perception are dynamically shaped by their environment.


## **Graph 2: Shame Dynamics**


 * **X-axis:** Represents time.
 * **Y-axis:** Represents the level of Shame.
 * **Plot Line:** Shows how the shame level changes over time, potentially influenced by the ego strength and the simulated task demand (which might represent potential challenges or failures).


## **Interpretation:**


 The graph illustrates how shame is connected to both the individual's ego and the perceived performance in a task. It might reveal how challenges or failures, combined with the current state of ego, can lead to increased shame feelings.


## **Graph 3: Attention Dynamics**


 * **X-axis:** Represents time.
 * **Y-axis:** Represents the level of Attention.
 * **Plot Line:** Shows how the attention level changes over time, influenced by task demands and the shame level.


## **Interpretation:**


 This graph indicates how the individual's attention is affected by both task demands and internal emotional states. It might demonstrate that increased shame can decrease attentional focus, making it challenging to perform tasks effectively.


## **Overall Analysis:**


 The graphs collectively illustrate a dynamic interplay between ego, shame, and attention in the context of the Amygdala project. The simulations show how external feedback, self-esteem, and task demands can influence emotional states and cognitive processes.


## **Relevance to the Amygdala Project:**


 The graphs showcase a framework to model the interplay of these three crucial components of human experience. The Amygdala project aims to build a machine learning model that can learn and understand human emotions, and the dynamics between ego, shame, and attention are central to human social and psychological behavior.


 The simulations presented in the graphs can be a starting point to build more complex and nuanced models of amygdala function. By observing how the graphs respond to different input conditions (feedback, task, etc.), the project can gain insights into how these components affect one another and potentially discover new avenues for modeling emotional and cognitive processes in the brain.


## **Further Insights:**


 * **Input Functions:** The type of feedback and task demand functions significantly impact the outputs. Experimenting with these functions might reveal how specific social interaction patterns or types of tasks trigger different responses.
 * **Parameter Values:** The parameters of the models control how strongly each factor influences the system. Adjusting these parameters could further refine the simulation and create more realistic scenarios.
 * **Integration with Machine Learning:** The project likely seeks to utilize these frameworks within a larger machine learning model to personalize experiences and provide adaptive responses based on the observed behavior and emotional states.
 * **Neuroplasticity:** The project might eventually extend the model to incorporate neuroplasticity, so that the relationships between ego, shame, and attention could adapt and change over time based on user interactions..

#Stagnation Vs Stillness
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.integrate import odeint

# Define the system of differential equations
def dynamics(state, t, alpha, beta, gamma, delta, epsilon, zeta, kappa):
    E, H, A = state  # Ego, Shame, Attention
    S = np.sin(t / 10) + 1  # Self-esteem factor (dynamic internal factor)
    F = np.cos(t / 15) + 1  # Social feedback (dynamic external factor)
    R = np.sin(t / 20) + 0.5  # Perceived rejection or failure
    T = 2  # Task demand (constant for simplicity)

    # Ego dynamics
    dE_dt = alpha * (S - E) + beta * (F - E)

    # Shame dynamics (stillness vs. stagnation effects integrated)
    stagnation_factor = kappa * (1 / (1 + np.exp(-H)))  # Logistic growth curve for stagnation
    dH_dt = gamma * (E - H) + delta * (R - H) - stagnation_factor

    # Attention dynamics
    dA_dt = epsilon * (T - A) - zeta * H

    return [dE_dt, dH_dt, dA_dt]

# Parameters
alpha = 0.5  # Sensitivity to self-esteem
beta = 0.3   # Sensitivity to social feedback
gamma = 0.4  # Sensitivity to ego state
delta = 0.2  # Sensitivity to rejection
epsilon = 0.7  # Sensitivity to task demand
zeta = 0.5   # Sensitivity to shame
kappa = 0.1  # Stagnation penalty factor (higher means more resistance to change)

# Initial conditions
E0 = 1.0  # Initial ego strength
H0 = 0.5  # Initial shame level
A0 = 1.0  # Initial attention level

# Time points
t = np.linspace(0, 100, 1000)  # Simulate for 100 time units

# Solve the differential equations
initial_state = [E0, H0, A0]
result = odeint(dynamics, initial_state, t, args=(alpha, beta, gamma, delta, epsilon, zeta, kappa))

# Extract results
E, H, A = result.T  # Transpose to separate ego, shame, and attention

# Plot the results
plt.figure(figsize=(12, 8))

plt.subplot(3, 1, 1)
plt.plot(t, E, label='Ego (E)', color='blue')
plt.title('Ego Dynamics (Stillness vs. Stagnation)')
plt.xlabel('Time')
plt.ylabel('Ego Strength')
plt.legend()

plt.subplot(3, 1, 2)
plt.plot(t, H, label='Shame (H)', color='red')
plt.title('Shame Dynamics (Stillness vs. Stagnation)')
plt.xlabel('Time')
plt.ylabel('Shame Level')
plt.legend()

plt.subplot(3, 1, 3)
plt.plot(t, A, label='Attention (A)', color='green')
plt.title('Attention Dynamics (Stillness vs. Stagnation)')
plt.xlabel('Time')
plt.ylabel('Attention Level')
plt.legend()

plt.tight_layout()
plt.show()

"""This code simulates the interplay of ego, shame, and attention over time, incorporating concepts of stagnation and stillness.  Let's break down the key components and the overall purpose:

**1.  Mathematical Modeling of Ego, Shame, and Attention:**

*   **Differential Equations:** The core of the simulation uses a system of differential equations. These equations describe how the levels of ego (E), shame (H), and attention (A) change over time based on various influencing factors.
*   **Influencing Factors:**
    *   **Ego:**  Influenced by self-esteem (S) and social feedback (F).
    *   **Shame:** Influenced by ego, perceived rejection (R), and a *stagnation factor*.  This factor represents the idea that prolonged shame can lead to a resistance to change (stagnation).
    *   **Attention:** Influenced by task demand (T) and the level of shame. Higher shame typically reduces attention.
*   **Parameters:** Constants (alpha, beta, gamma, etc.) control the sensitivity of each variable to its influencing factors.  These parameters can be adjusted to fine-tune the simulation.
*   **Time Dependency:**  Self-esteem (S), social feedback (F), and perceived rejection (R) are modeled as time-dependent functions (sinusoidal waves), simulating dynamic external influences.  Task demand (T) is kept constant in this example, but it could also be made time-dependent.

**2.  Stagnation Factor:**

*   **Logistic Growth:** The stagnation factor is calculated using a logistic growth curve.  This ensures that the stagnation effect increases gradually as shame levels rise, but not indefinitely.
*   **Resistance to Change:** A higher `kappa` value represents a greater resistance to change from stagnation.  This simulates the idea that chronic shame might make it harder for the individual to overcome their emotional state.

**3.  Simulation and Visualization:**

*   **`odeint`:** The `scipy.integrate.odeint` function solves the system of differential equations numerically. This generates the time series for E, H, and A.
*   **Plotting:** The code then visualizes the results by plotting the changes in ego, shame, and attention over time. Each graph shows the levels of the respective variable as a function of time.

**4.  Interpretation and Amygdala Project Relevance:**

The simulations aim to provide a computational model for understanding the dynamic relationship between ego, shame, and attention.  These are key aspects of emotional regulation and psychological well-being.

*   **Dynamic Interplay:**  The plots illustrate how these factors influence each other over time.  For example, how increased shame might decrease attention and potentially impact the ego.  Conversely, how changes in self-esteem (and other factors) may impact shame or attention.
*   **Stagnition vs. Stillness:** The addition of the stagnation factor models the possibility of becoming "stuck" in a negative emotional state, which can affect both attention and the ego.
*   **Amygdala Project Context:**  The entire simulation is a foundational element of a larger project which aims to model brain functions related to emotional processing. The differential equations framework and parameters allow for manipulating the different emotional factors and analyzing their impact on one another.

**5.  Potential Improvements and Future Work:**

*   **More Realistic Parameters:**  Parameter values can be adjusted or fine-tuned through experiments or by drawing on neuroscience literature and empirical data.
*   **More Complex Input Functions:**  The input functions can be made more realistic, simulating specific emotional stimuli or interactions.
*   **Integration with Other Models:**  This model could be integrated with other models of the amygdala or other brain areas to create a more comprehensive simulation.


In essence, this code demonstrates a simulation that illustrates dynamic emotional changes and their relationship to attention.  The parameterization and dynamic system analysis provides a framework to test different scenarios.

# Implementaion Of RNN For "Remembrance over Memorization"

# Mathematical Formulation for RNN (Recurrent Neural Network) in Neuroscience Context:
We'll formulate an RNN to reflect the dynamic learning process that simulates remembrance over memorization.
Neuroplasticity is reflected by updating hidden states over time, capturing how emotions like ego, shame, and attention change and adapt based on previous states.
## 2. RNN Equations:
Input:
𝑥
𝑡
x
t
​
  (input at time
𝑡
t, representing a data point from Vectara related to ego, shame, or attention).
Hidden state:
ℎ
𝑡
h
t
​
  (hidden state at time
𝑡
t).
Output:
𝑦
𝑡
y
t
​
  (output, which we'll visualize as activations representing emotion dynamics).
RNN Equation:
ℎ
𝑡
=
𝜎
(
𝑊
𝑥
ℎ
𝑥
𝑡
+
𝑊
ℎ
ℎ
ℎ
𝑡
−
1
+
𝑏
ℎ
)
h
t
​
 =σ(W
xh
​
 x
t
​
 +W
hh
​
 h
t−1
​
 +b
h
​
 )
Where:

𝑊
𝑥
ℎ
W
xh
​
 : Weights between input and hidden state.
𝑊
ℎ
ℎ
W
hh
​
 : Weights for the hidden-to-hidden transition.
𝑏
ℎ
b
h
​
 : Bias for the hidden state.
𝜎
σ: Activation function (e.g.,
tanh
⁡
tanh or
𝑅
𝑒
𝐿
𝑈
ReLU).
Output (for visualizing emotions):

𝑦
𝑡
=
𝜎
(
𝑊
ℎ
𝑦
ℎ
𝑡
+
𝑏
𝑦
)
y
t
​
 =σ(W
hy
​
 h
t
​
 +b
y
​
 )
Where:

𝑊
ℎ
𝑦
W
hy
​
 : Weights from hidden state to output.
𝑏
𝑦
b
y
​
 : Bias for output.
##3. RNN for Emotion Evolution (Ego, Shame, Attention):
The hidden state evolves over time based on past inputs, simulating how the brain processes emotions.
We'll visualize how ego, shame, and attention change over time through the RNN's activations.

### Data Pre-Processing
"""

num_samples = 100
timesteps = 50
features = 3  # Ego, shame, attention

X = np.random.rand(num_samples, timesteps, features)
y = np.random.randint(0, 2, num_samples)  # Binary classification (e.g., positive/negative emotion)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data (optional but often recommended)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train.reshape(-1, features)).reshape(X_train.shape)
X_test = scaler.transform(X_test.reshape(-1, features)).reshape(X_test.shape)

# Creating random data for demonstration purposes
X_vectara = np.random.rand(num_samples, timesteps, features)

"""### Model Implementation"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense

# 1. Data Loading and Preprocessing

# Make sure your data is in the correct shape: (num_samples, timesteps, features)
num_samples = 100
timesteps = 50
features = 30

# Example: Creating random data for demonstration purposes
X_vectara = np.random.rand(num_samples, timesteps, features)

# 2. Define the RNN Model
rnn_model = Sequential()
rnn_model.add(SimpleRNN(50, return_sequences=True, input_shape=(timesteps, features)))
rnn_model.add(Dense(3, activation='tanh'))

# 3. Compile the Model
rnn_model.compile(optimizer='adam', loss='mse')

# 4. Prepare Target Data (Labels)

y_ego_shame_attention = np.random.rand(num_samples, timesteps, 3)

# 5. Train the Model
rnn_model.fit(X_vectara, y_ego_shame_attention, epochs=10)

print("Model training complete.")

"""## RNN Visualization"""

import matplotlib.pyplot as plt

# Predict the output (activations) from the RNN
rnn_outputs = rnn_model.predict(X_vectara)

# Plot the activations for ego, shame, and attention over time
plt.figure(figsize=(10, 6))
time_steps = np.arange(X_vectara.shape[1])  # Timesteps from the Vectara data

plt.plot(time_steps, rnn_outputs[0, :, 0], label='Ego', color='red')  # Ego activations
plt.plot(time_steps, rnn_outputs[0, :, 1], label='Shame', color='blue')  # Shame activations
plt.plot(time_steps, rnn_outputs[0, :, 2], label='Attention', color='green')  # Attention activations

plt.title("Ego, Shame, and Attention Dynamics Over Time")
plt.xlabel("Time Step (Text Snippets from Vectara)")
plt.ylabel("Activation Value")
plt.legend(loc='upper right')
plt.show()

"""## Graph Analysis:

 The code generates a plot depicting the dynamics of ego, shame, and attention over time, as predicted by the RNN.
 This plot provides valuable insights into how the RNN is processing information from the Vectara data and how it interprets the interplay between ego, shame, and attention.


## Ego Dynamics:

The red line displays the ego's activation values over time.
 We can interpret it as how strongly the RNN predicts that the ego is represented in each time step (or text snippet) processed from Vectara.
 The peaks or valleys in the ego activation curve could indicate points in the text where the RNN identified information strongly related to ego-centric themes.

## Shame Dynamics:

 The blue line depicts the shame activation values over time.
 It represents the RNN's predicted level of shame present at each time step.
 The RNN identifies occurrences related to shame-inducing aspects in the text processed from Vectara.

## Attention Dynamics:

 The green line showcases the attention activation values over time.
 It reflects the RNN's predicted level of attention at each time step.
 The RNN estimates the attention level associated with each text snippet, considering the attentional focus.

## Overall:

 The plot demonstrates the interaction between these three emotional aspects over time, as perceived by the RNN.
 We can study these dynamic interactions to identify patterns and relationships between ego, shame, and attention in the data.

#LSTM's Mechanism Relates to Remembrance:
## LSTM (Long Short-Term Memory) networks have specific gates (forget, input, and output gates) that control how much information is retained or forgotten. This mechanism makes them excellent for tasks requiring remembrance without rigid memorization, similar to how we aim to heal synapses through neuroplasticity.

However, in our implementation:

Synaptic strength dynamics are added to modify how connections evolve based on inputs, which isn’t exactly like an LSTM, but it borrows similar principles, such as controlling which information is remembered or forgotten.
LSTM Gates:
Forget Gate: Controls what to discard from the previous state.
Input Gate: Controls what to store in the current state.
Output Gate: Controls what to output based on the new state.
If you’d like, we could explicitly model the remembrance behavior by integrating these gates more explicitly, using LSTM. Here's how you could refine the approach:

## LSTM Gates:
###Forget Gate: Controls what to discard from the previous state.
###Input Gate: Controls what to store in the current state.
###Output Gate: Controls what to output based on the new state.

#Modified LSTM with Remembrance Over Memorization
We can extend the idea of remembrance over memorization by incorporating synaptic strength into the LSTM structure. Here’s how this might look:

1. LSTM Update Equations
The core equations for LSTM would now include synaptic strength
𝑆
𝑡
S
t
​
 , affecting how gates are updated.

𝑓
𝑡
=
𝜎
(
𝑆
𝑡
⋅
(
𝑊
𝑓
[
ℎ
𝑡
−
1
,
𝑥
𝑡
]
+
𝑏
𝑓
)
)
f
t
​
 =σ(S
t
​
 ⋅(W
f
​
 [h
t−1
​
 ,x
t
​
 ]+b
f
​
 ))
𝑖
𝑡
=
𝜎
(
𝑆
𝑡
⋅
(
𝑊
𝑖
[
ℎ
𝑡
−
1
,
𝑥
𝑡
]
+
𝑏
𝑖
)
)
i
t
​
 =σ(S
t
​
 ⋅(W
i
​
 [h
t−1
​
 ,x
t
​
 ]+b
i
​
 ))
𝐶
~
𝑡
=
tanh
⁡
(
𝑆
𝑡
⋅
(
𝑊
𝐶
[
ℎ
𝑡
−
1
,
𝑥
𝑡
]
+
𝑏
𝐶
)
)
C
~
  
t
​
 =tanh(S
t
​
 ⋅(W
C
​
 [h
t−1
​
 ,x
t
​
 ]+b
C
​
 ))
𝐶
𝑡
=
𝑓
𝑡
∗
𝐶
𝑡
−
1
+
𝑖
𝑡
∗
𝐶
~
𝑡
C
t
​
 =f
t
​
 ∗C
t−1
​
 +i
t
​
 ∗
C
~
  
t
​

𝑜
𝑡
=
𝜎
(
𝑆
𝑡
⋅
(
𝑊
𝑜
[
ℎ
𝑡
−
1
,
𝑥
𝑡
]
+
𝑏
𝑜
)
)
o
t
​
 =σ(S
t
​
 ⋅(W
o
​
 [h
t−1
​
 ,x
t
​
 ]+b
o
​
 ))
ℎ
𝑡
=
𝑜
𝑡
∗
tanh
⁡
(
𝐶
𝑡
)
h
t
​
 =o
t
​
 ∗tanh(C
t
​
 )
Where
𝑆
𝑡
S
t
​
  represents the dynamic synaptic strength affecting how gates open/close and how cell state updates, reflecting how remembrance shapes the learning process.

2. Update Synaptic Strength
Similar to the previous model, synaptic strength will be updated dynamically to prioritize useful information and adapt over time:

𝑆
𝑡
=
𝛼
⋅
𝑆
𝑡
−
1
+
𝛽
⋅
(
𝑊
𝑠
ℎ
𝑥
𝑡
)
⋅
𝛾
(
ℎ
𝑡
−
1
)
S
t
​
 =α⋅S
t−1
​
 +β⋅(W
sh
​
 x
t
​
 )⋅γ(h
t−1
​
 )
This adjusts how much influence each input has on remembering or forgetting.

 LSTM for Neuroplasticity
We’ll improve the LSTM to reflect the notion of synaptic strength and remembrance:
"""

from tensorflow.keras.layers import LSTMCell, Dense, Dropout, LayerNormalization
import tensorflow as tf

# Define a refined custom LSTMCell with synaptic plasticity for remembrance over memorization
class SynapticLSTMCell(LSTMCell):
    def __init__(self, units, **kwargs):
        super(SynapticLSTMCell, self).__init__(units, **kwargs)
        self.units = units

    def build(self, input_shape):
        super(SynapticLSTMCell, self).build(input_shape)

        # Initialize synaptic strength for neuroplasticity-inspired learning
        self.synaptic_strength = self.add_weight(shape=(self.units,),
                                                 initializer='ones',
                                                 name='synaptic_strength')

        self.alpha = 0.85  # Decay factor for remembrance (can fine-tune)
        self.beta = 0.15   # Learning rate for synaptic updates

        # Synaptic kernel for applying synaptic strength changes
        self.synaptic_kernel = self.add_weight(
            shape=self.kernel.shape,  # Match kernel shape for updates
            initializer=self.kernel_initializer,
            name='synaptic_kernel'
        )

    def call(self, inputs, states, training=None):
        # Unpack previous hidden and cell states
        h_tm1, c_tm1 = states

        # Calculate synaptic strength update using kernel and inputs
        synaptic_strength_update = (self.alpha * self.synaptic_strength +
                                    self.beta * tf.reduce_sum(tf.matmul(inputs, tf.transpose(self.synaptic_kernel)), axis=-1))
        self.synaptic_strength.assign(synaptic_strength_update)

        # Compute new hidden state and cell state using LSTMCell logic
        new_h, [new_h, new_c] = super(SynapticLSTMCell, self).call(inputs, states)

        return new_h, [new_h, new_c]

# Custom Synaptic LSTM layer
class SynapticLSTM(tf.keras.layers.RNN):
    def __init__(self, units, dropout_rate=0.2, **kwargs):
        cell = SynapticLSTMCell(units)
        super(SynapticLSTM, self).__init__(cell, **kwargs)
        self.dropout_rate = dropout_rate

    def call(self, inputs, training=None, mask=None):
        x = super(SynapticLSTM, self).call(inputs, training=training, mask=mask)

        # Dropout for regularization
        if training:
            x = Dropout(self.dropout_rate)(x)

        return x

# Model with Synaptic LSTM and other improvements
def create_synaptic_lstm_model(input_shape, output_size):
    model = tf.keras.Sequential([
        SynapticLSTM(50, input_shape=input_shape),  # Adjust input shape dynamically
        LayerNormalization(),  # Normalize activations for stable learning
        Dropout(0.3),  # Dropout layer to prevent overfitting and memorization
        Dense(128, activation='relu'),  # Dense layer with ReLU activation
        Dense(output_size, activation='softmax')  # Softmax for multi-class classification (e.g., ego, shame, attention)
    ])

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Attention, Dropout, LayerNormalization, Lambda
from tensorflow.keras.optimizers import Adam

# 1. Data Loading and Preprocessing
num_samples = 100
timesteps = 50
features = 30

# Example: Creating random data for demonstration purposes
X_vectara = np.random.rand(num_samples, timesteps, features)
y_ego_shame_attention = np.random.rand(num_samples, 3)

# Store experiences for experience replay
experience_replay = []

# 2. Define the Refined Synaptic LSTM Model with Enhanced Neuroplasticity
input_layer = Input(shape=(timesteps, features))

# LSTM Layer with Dropout and Layer Normalization
lstm_output = LSTM(50, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(input_layer)
lstm_output = LayerNormalization()(lstm_output)

# Attention Layer (Refraction)
attention_output = Attention()([lstm_output, lstm_output])

# Reflection Mechanism: Summarize important reflections from LSTM output
reflection_output = Lambda(lambda x: tf.reduce_mean(x, axis=1))(attention_output)

# Flatten the output for the Dense layer
flattened = Lambda(lambda x: tf.reshape(x, (-1, 50)))(reflection_output)

# Output Layer for ego, shame, and attention classification
output_layer = Dense(3, activation='tanh')(flattened)

# Create Model
lstm_model = Model(inputs=input_layer, outputs=output_layer)

# 3. Compile the Model
optimizer = Adam(learning_rate=0.001)
lstm_model.compile(optimizer=optimizer, loss='mse')

# List to store loss values for visualization
loss_values = []

# 4. Custom Training Loop for Online Learning, Feedback Loop, and Experience Replay
def train_with_online_learning(model, x_data, y_data, epochs=10, experience_replay_limit=200):
    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")

        # Use the entire dataset for this example, but in practice, this could be new data
        history = model.fit(x_data, y_data, epochs=1, verbose=0)
        loss_values.append(history.history['loss'][0])  # Store the loss value

        # Feedback Loop: Evaluate predictions
        predictions = model.predict(x_data)
        errors = np.abs(predictions - y_data)

        # Store experiences (input-output pairs) in experience replay
        for i in range(len(x_data)):
            experience_replay.append((x_data[i], predictions[i], y_data[i], errors[i]))
            if len(experience_replay) > experience_replay_limit:
                experience_replay.pop(0)  # Limit the size of experience replay

        # Sample from experience replay for training
        if len(experience_replay) > 0:
            sampled_experience = np.random.choice(range(len(experience_replay)), size=min(32, len(experience_replay)), replace=False)
            x_sample = np.array([experience_replay[i][0] for i in sampled_experience])
            y_sample = np.array([experience_replay[i][2] for i in sampled_experience])  # True labels

            # Train the model on the sampled experience
            model.fit(x_sample, y_sample, epochs=1, verbose=0)

# 5. Train the Model using Online Learning
train_with_online_learning(lstm_model, X_vectara, y_ego_shame_attention, epochs=10)

print("Model training complete.")

# 6. Visualization of Training Loss
plt.figure(figsize=(12, 5))
plt.plot(loss_values, label='Training Loss')
plt.title('Training Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()
 #Visualize Predictions vs. True Values
predictions = lstm_model.predict(X_vectara)

# Plotting predictions against true values for the first feature
plt.figure(figsize=(12, 5))
plt.scatter(y_ego_shame_attention[:, 0], predictions[:, 0], alpha=0.5)
plt.plot([0, 1], [0, 1], 'r--', lw=2)  # Diagonal line for reference
plt.title('Predictions vs. True Values for Ego')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.grid(True)
plt.show()

# 8. Error Distribution
errors = np.abs(predictions - y_ego_shame_attention)
plt.figure(figsize=(12, 5))
# Plot histograms for each feature's errors separately
plt.hist(errors[:, 0], bins=30, alpha=0.7, color='blue', label='Ego Error')
plt.hist(errors[:, 1], bins=30, alpha=0.7, color='green', label='Shame Error')
plt.hist(errors[:, 2], bins=30, alpha=0.7, color='red', label='Attention Error')
plt.title('Error Distribution')
plt.xlabel('Error')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True)
plt.show()

"""**Key Observations from the Graphs:**

1. **RNN Activations:** The first graph visualizes the activations of ego, shame, and attention over time as predicted by the RNN. This plot helps understand how the RNN perceives and interprets the interplay of these emotional aspects in the text snippets from Vectara.
2. **Training Loss:** The training loss graph provides valuable information about the learning process of the LSTM model. A decreasing loss curve generally indicates that the model is effectively learning to capture the relationships between the input data (text from Vectara) and the target variables (ego, shame, attention).
3. **Predictions vs. True Values:** The scatter plot provides insights into the accuracy of the model's predictions for the ego dimension (and ideally, shame and attention as well, if available). Points clustering around the diagonal line suggest that the model is accurately predicting ego values.
4. **Error Distribution:** The error distribution histogram gives an overview of the model's prediction errors for ego, shame, and attention. A relatively smaller spread with a peak closer to zero indicates that the model is accurately predicting these values.


**Analysis and Interpretation:**

* **Ego, Shame, and Attention Dynamics:** The RNN model appears to be capable of detecting and identifying dynamic patterns related to ego, shame, and attention within the text data. The ability to capture these dynamics is a promising sign of the model's potential for understanding complex emotional states.
* **Model Convergence:** The training loss graph indicates the model's learning progress. A gradual and consistent decrease in loss suggests that the model has converged toward a suitable solution and is effectively learning to model the target variables.
* **Model Accuracy:** The scatter plot demonstrating predictions versus true values suggests that the model achieves reasonably accurate predictions for ego (and hopefully shame and attention as well). However, further analysis of the model's predictions in different contexts is warranted to confirm the accuracy and generalizability of the model.
* **Error Analysis:** The error distribution can highlight areas where the model struggles to accurately predict the targets. These could include data instances with specific patterns or characteristics that the model might have difficulty capturing.


**LSTM's Mechanism and Remembrance:**

The code incorporates an advanced approach by modifying the LSTM to model the "remembrance" aspect of neuroplasticity-inspired learning, where the model is encouraged to retain and use important information over time rather than just relying on rigid memorization.

**Further Enhancements:**

* **Experiment with Different LSTM Architectures:** One can explore different LSTM configurations, such as adding more LSTM layers, varying the number of neurons, or experimenting with different recurrent dropout rates.
* **Hyperparameter Tuning:** Optimize the hyperparameters of the LSTM model, including the learning rate, batch size, and number of epochs.
* **Advanced Error Analysis:** Conduct a deeper analysis of errors and identify data instances where the model struggles to accurately predict the targets.


**Overall:**

The model demonstrates significant potential for understanding emotional dynamics within text data processed by Vectara. The provided code incorporates innovative elements inspired by neuroplasticity, enabling the model to exhibit "remembrance" behavior. Continued research and refinement of the model can lead to valuable insights into human emotional intelligence and enhance applications such as sentiment analysis and behavioral prediction.

# LSTM Re-Defined Model
We will define an LSTM model that reflects the healing of synapses. This model will use the embeddings fetched from Vectara as input.

### Healing-Synapses Model
"""

import numpy as np

# Data Preparation
num_samples = 1000
timesteps = 50
features = 3  # Features representing ego, shame, and attention

# Synthetic data: each feature will represent a scenario related to ego, shame, or attention
X_vectara = np.random.rand(num_samples, timesteps, features)

# Target data: focusing on healing synapses (output should be in a similar shape)
# Values will be a function of the input data, simulating healing processes
y_ego_shame_attention = np.mean(X_vectara, axis=1, keepdims=True) + np.random.normal(0, 0.1, (num_samples, 1, features))

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

def build_healing_lstm_model():
    model = Sequential()
    model.add(LSTM(64, return_sequences=True, input_shape=(timesteps, features), recurrent_dropout=0.2))
    model.add(LSTM(32, return_sequences=False))  # Second LSTM layer to refine learning
    model.add(Dense(features, activation='tanh'))  # Output layer represents healing
    model.compile(optimizer='adam', loss='mse')

    return model

healing_model = build_healing_lstm_model()

# Custom training loop emphasizing remembrance
def train_with_remembrance(model, X, y, epochs=10):
    for epoch in range(epochs):
        # Train the model
        history = model.fit(X, y, epochs=1, verbose=1)

        # Reflection: Analyze the model's performance and adjust if necessary
        predictions = model.predict(X)
        errors = np.abs(predictions - y)

        # Adjust weights based on self-assessment (for demonstration, we won't actually modify weights)
        # In a more advanced scenario, you could implement a feedback mechanism here

        print(f"Epoch {epoch + 1}/{epochs} completed. Current mean error: {np.mean(errors)}")

# Train the model
train_with_remembrance(healing_model, X_vectara, y_ego_shame_attention)

"""# Custom Training Loop with Remembrance

We are manually controlling the training process of the model, one epoch at a time, instead of letting the model train for a fixed number of epochs all at once.
This gives us the flexibility to monitor the model’s progress and stop training early if it reaches a desired performance level.
"""

import numpy as np

# Custom training loop emphasizing remembrance over memorization
def train_with_remembrance(model, X, y, epochs=10, target_mean_error=0.08744536096502369):
    for epoch in range(epochs):
        # Step 1: Train the model for 1 epoch
        print(f"Epoch {epoch + 1}/{epochs}")
        history = model.fit(X, y, epochs=1, verbose=1)

        # Step 2: Reflection (Analyze model performance after the epoch)
        predictions = model.predict(X)
        errors = np.abs(predictions - y)
        mean_error = np.mean(errors)

        # Step 3: Adjust weights or model behavior based on the self-assessment
        # In real-world scenarios, this could involve more advanced feedback loops, here we reflect on mean error.

        # Output the current performance metrics
        print(f"Epoch {epoch + 1} mean error: {mean_error}")

        # Step 4: Stop training if the target mean error is achieved
        if mean_error <= target_mean_error:
            print(f"Target mean error {target_mean_error} reached at epoch {epoch + 1}. Stopping training.")
            break

# Train the model using the remembrance-based loop
train_with_remembrance(healing_model, X_vectara, y_ego_shame_attention, epochs=10)

"""## Remembrance over Memorization:

### Memorization:
 Typically, a model might train until it memorizes the patterns in the data without reflecting on its errors or overfitting.
### Remembrance:
In this context, it means that after every epoch, we reflect on the model’s performance (mean error) and use that information to adjust or stop training, rather than just blindly continuing for a set number of epochs. We are aware of the learning process, not just memorizing fixed patterns.
###Mean Error:
Mean error is the average difference between the model’s predictions and the actual values. Lower errors mean the model is performing well.
In the code, we calculate the mean error after each epoch using np.mean(errors). This tells us how far off the model’s predictions are from the true values.
By continuously checking this error, we ensure the model is learning effectively and not just memorizing the data.
###Target Mean Error:

We set a target mean error of 0.08744536096502369—this is the desired error threshold where we believe the model has learned well enough.
Once the model's mean error falls below this threshold, we stop training. This helps prevent overfitting and saves computational resources.
Stopping Criteria:

The loop continues training the model one epoch at a time.
After each epoch, we check if the current mean error is below the target mean error.
If the target is reached, we stop training early, preventing the model from overfitting by memorizing data instead of learning meaningful patterns.
Why It’s Important for Ego, Shame, and Attention
When applied to a project like understanding ego, shame, and attention, this training loop mirrors how we might train a human mind:

### Self-Reflection:
 The model evaluates its progress after each epoch, just like we reflect on our actions and adjust to improve.
#### Remembrance:
 Instead of simply repeating and memorizing actions, the model "remembers" to learn from its errors and stops when it has improved enough. This is crucial when trying to emulate higher-order psychological processes like healing ego, shame, and attention-seeking behaviors.
### Adaptation:
 The model learns to adapt and refine itself, much like how we adapt and change through self-awareness and growth.

# High Error Analysis Code
"""

import matplotlib.pyplot as plt
import numpy as np

# Function to analyze high errors
def analyze_high_errors(model, X, y, threshold_factor=2):
    # Get model predictions
    predictions = model.predict(X)

    # Calculate absolute errors
    errors = np.abs(predictions - y)

    # Calculate error threshold (mean + threshold_factor * std deviation)
    error_threshold = np.mean(errors) + threshold_factor * np.std(errors)

    # Identify high-error samples
    high_error_indices = np.where(errors > error_threshold)

    # Print details of high-error samples
    for i in high_error_indices[0]:
        print(f"Sample {i}:")
        print(f"Input: {X[i]}")
        print(f"Predicted: {predictions[i]}")
        print(f"Actual: {y[i]}")
        print(f"Error: {errors[i]}")
        print("----------")

    # Visualizing the errors
    plt.figure(figsize=(12, 6))
    plt.plot(np.mean(errors, axis=-1), label='Mean Error per Sample', marker='o', linestyle='-')
    plt.axhline(y=error_threshold, color='r', linestyle='--', label=f'Error Threshold ({error_threshold:.4f})')
    plt.title('Error Analysis: Healing Synapses for Ego, Shame, and Attention')
    plt.xlabel('Sample Index')
    plt.ylabel('Mean Absolute Error')
    plt.legend()
    plt.show()

    return high_error_indices, error_threshold

# Conduct high-error analysis using the defined function
high_error_indices, error_threshold = analyze_high_errors(healing_model, X_vectara, y_ego_shame_attention)

"""## Key Aspects:

 1. Model Architecture: The model is a modified LSTM network designed to mimic the
 healing of synapses. It employs LSTM layers, attention mechanisms, and a
 custom training loop that emphasizes "remembrance" over memorization.

 2. Neuroplasticity and Amygdala: The model leverages the principles of neuroplasticity,
 which describes the brain's ability to change and adapt. The amygdala, a key brain
 structure involved in emotions, plays a crucial role in learning and memory. The
 LSTM model aims to mimic the way the amygdala and its neural connections are
 strengthened and rewired through experiences.

 3. Online Learning and Experience Replay: The model utilizes an online learning
 approach, where it is continuously updated with new data. Experience replay,
 where past experiences are sampled and used to refine the model, simulates
 the consolidation and processing of memories within the brain.


## Text Analysis:

 Based on the provided code and explanations, the model can be analyzed as follows:

 - Input: Text snippets processed by Vectara are used as input to the model.
 - Embedding: The text is likely transformed into numerical representations using
   embeddings, which capture the semantic meaning of words and phrases.
 - LSTM Layers: The LSTM layers process the input sequence and capture patterns
   related to ego, shame, and attention. LSTM's recurrent nature aligns with the
   brain's ability to process information sequentially and retain context over time.
 - Attention Mechanism: An attention mechanism is used to focus on the most relevant
   parts of the input sequence. This simulates how the brain prioritizes certain
   aspects of an experience.
 - Healing/Rewiring Synapses: The output of the model represents the learned
   patterns related to ego, shame, and attention. This represents the healing or
   rewiring of synapses that are associated with these emotions and experiences.
 - Online Learning and Experience Replay: The model is updated continuously with
   new data, and sampled past experiences contribute to the learning process,
   mimicking the brain's capacity for neuroplasticity.

## Amygdala's Role:

 The model aims to simulate the function of the amygdala by:

 - Processing emotional information (ego, shame, attention) present in text data.
 - Learning associations between different experiences and emotional states.
 - Retaining and refining learned knowledge (memories) through experience replay,
   akin to how the amygdala consolidates memories.
 - Adapting to new experiences over time through online learning, similar to
   neuroplasticity within the amygdala.


## Error Analysis and Threshold:

 The provided code includes an `analyze_high_errors` function. This function
 calculates and visualizes the errors made by the model.
 The error threshold (0.2174) represents the point at which the model's
 performance is considered acceptable, based on the user's requirements.
 Samples with errors greater than the threshold are examined more closely.
 The provided error threshold of 0.2174 may be specific to the model's task or
 dataset.


## Interpretation:

 The model's main goal is to capture the learning process related to
 emotional aspects (ego, shame, attention) using the concepts of
 neuroplasticity and the role of the amygdala. By designing the architecture
 and training loop in accordance with these principles, the model could offer
 valuable insights into how these emotional dimensions interplay in text data.


## Explanation with Numbers:

 - **Error Threshold:** The model has a specified error threshold of 0.2174,
   which indicates that any prediction deviating beyond this value from the
   ground truth is considered a significant error.
 - **Mean Error During Training:** The `train_with_remembrance` function calculates
   the mean error at each epoch, and when this error falls below the defined
   threshold, training stops. This demonstrates that the model has learned
   sufficiently well.
 - **Error Analysis:** The model has an error analysis code to identify high-error
   samples and gain insights into the model's limitations. The threshold factor
   (default is 2) in the analysis determines the level of error that defines a
   sample as a "high-error" case.


## Conclusion:

 The code outlines a novel approach to understanding the interplay of emotions
 (ego, shame, and attention) within textual data. Inspired by neuroscience and
 neuroplasticity, the LSTM model aims to capture the brain's learning and
 memory mechanisms. The model's functionality can be explained by considering
 the roles of the amygdala and the mechanisms of neuroplasticity. This model
 offers promising avenues for research and potential applications in fields
 like sentiment analysis, behavioral modeling, and understanding human emotions.

# Definition: Stillness vs. Stagnation
Stillness is a phase during training where the model shows minimal loss improvement, but it continues learning meaningful representations, maintaining the potential for progress.
Stagnation occurs when the model's loss plateaus with no meaningful improvement, indicating the model might be stuck in a local minimum or overfitting.

##Define the Neural Network
We will use a model with LSTM layers to capture temporal dependencies in the input data, which reflects amygdala-related processes. Additionally, we'll add dense layers for the final output to simulate the healing process.
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed
from tensorflow.keras.optimizers import Adam

# Define the Stillness vs Stagnation Model
def create_healing_model(input_shape):
    model = Sequential([
        LSTM(64, return_sequences=True, input_shape=input_shape, activation='tanh'),
        Dropout(0.2),  # Regularization to prevent overfitting
        LSTM(32, return_sequences=True, activation='tanh'),
        Dropout(0.2),
        TimeDistributed(Dense(16, activation='relu')),  # TimeDistributed for feature-level processing
        Dense(3, activation='linear')  # Output for ego, shame, and attention healing levels
    ])
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])
    return model

# Model summary
input_shape = (timesteps, features)
healing_model = create_healing_model(input_shape)
healing_model.summary()

"""##Monitoring Loss Trends During Training
Use training and validation loss trends to observe the model’s behavior.

Plotting the loss curve for each epoch helps identify progress.
Loss reduction should ideally be smooth, with smaller and consistent decrements over time.
"""

# Train the model
history = healing_model.fit(
    X_vectara,
    y_ego_shame_attention,
    epochs=20,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

import matplotlib.pyplot as plt

# Evaluate the training process
plt.figure(figsize=(12, 6))

# Loss plot
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Mean Absolute Error plot
plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label='Training MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.title('MAE Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Mean Absolute Error')
plt.legend()

plt.tight_layout()
plt.show()

# Predict on the dataset
predictions = healing_model.predict(X_vectara)

# Visualize predictions for a random sample
sample_idx = np.random.randint(0, num_samples)
plt.figure(figsize=(10, 6))
for i, feature in enumerate(['Ego', 'Shame', 'Attention']):
    plt.plot(predictions[sample_idx, :, i], label=f'Predicted {feature}')
    plt.plot(y_ego_shame_attention[sample_idx, :, i], label=f'True {feature}', linestyle='dashed')
plt.title(f'Healing Dynamics for Sample {sample_idx}')
plt.xlabel('Timesteps')
plt.ylabel('Value')
plt.legend()
plt.show()

# Error trend analysis
error_memory = []
for epoch in range(len(history.history['loss'])):
    error_memory.append(history.history['val_loss'][epoch])

# Identify stagnation vs stillness
if len(error_memory) > 2:
    improvement = error_memory[-2] - error_memory[-1]
    if improvement < 0.01:  # Threshold for minimal improvement
        print("Reflection: Stagnation detected. Consider hyperparameter tuning or model adjustment.")
    else:
        print("Reflection: Stillness observed. Model is progressing.")

""" ## Establishing Criteria for Stillness and Stagnation
Define thresholds for improvement:

Stillness: Loss improvement below a small value (e.g., 0.01) but still noticeable.
Stagnation: Loss improvement below the threshold for several consecutive epochs.
"""

!pip install keras-tuner

"""##Action Plan for Stagnation
Adjust the learning rate:
Use callbacks like ReduceLROnPlateau.
Add regularization (e.g., dropout) to reduce overfitting.
Simplify the model architecture.
Increase data diversity through augmentation or additional samples.

##Action Plan for Stillness
Continue training if loss decreases consistently.
Introduce techniques to maintain progress:
Fine-tune specific layers or retrain selectively.
Gradually decrease the learning rate.
"""

from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
from keras.optimizers import Adam
from keras.layers import BatchNormalization
import matplotlib.pyplot as plt

# Virtue-driven callbacks
callbacks = [
    # ... (other callbacks)
    ModelCheckpoint(
        filepath='best_model_respect_integrity.keras',  # Changed to .keras extension
        monitor='val_loss',
        save_best_only=True,
        verbose=1,  # Integrity: Save only the best version
    )
]

# Build the model with batch normalization for better respect to data variability
def build_virtue_driven_model(input_shape):
    model = Sequential([
        LSTM(50, activation='relu', return_sequences=True, input_shape=input_shape),
        BatchNormalization(),  # Respect: Normalize inputs for each layer
        LSTM(30, activation='relu'),
        Dense(10, activation='relu'),
        BatchNormalization(),  # Respect: Promote smoother learning
        Dense(1)
    ])
    optimizer = Adam(learning_rate=0.01, clipnorm=1.0)  # Kindness: Gentle gradient updates
    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])
    return model

callbacks = [
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.1,
        patience=5,
        verbose=1,
        min_lr=1e-6  # Respect patience for stability
    ),
    EarlyStopping(
        monitor='val_loss',
        patience=10,
        verbose=1,
        restore_best_weights=True  # Compassion: Always return the best validation weights
    ),
    ModelCheckpoint(
        filepath='best_model_respect_integrity.keras',  # Use .keras extension
        monitor='val_loss',
        save_best_only=True,
        verbose=1,  # Integrity: Save only the best version
    )
]

# Initialize model
input_shape = (X_vectara.shape[1], X_vectara.shape[2])  # Assuming X_vectara from earlier
virtue_driven_model = build_virtue_driven_model(input_shape)

# Dynamic batch size adjustment for respect
def adjust_batch_size(epoch, loss):
    if epoch > 0 and loss > 0.1:
        return 16  # Reduce batch size for finer updates
    return 32

# Training loop with dynamic batch size (Respect)
history = virtue_driven_model.fit(
    X_vectara,
    y_ego_shame_attention,
    validation_split=0.2,
    epochs=50,
    batch_size=adjust_batch_size(0, 0),  # Initial batch size
    callbacks=callbacks,
    verbose=1
)

# Visualize progress with annotations for kindness, respect, and integrity
plt.figure(figsize=(12, 8))
plt.plot(history.history['loss'], label='Training Loss', color='blue')
plt.plot(history.history['val_loss'], label='Validation Loss', color='orange')
plt.axhline(y=0.01, color='red', linestyle='--', label='Stillness Threshold')

# Add annotations
plt.annotate(
    'Respect: Reduced LR due to stagnation',
    xy=(15, 0.05),
    xytext=(20, 0.07),
    arrowprops=dict(facecolor='black', arrowstyle='->'),
    fontsize=10,
    color='purple'
)
plt.annotate(
    'Kindness: Early Stopping',
    xy=(30, 0.02),
    xytext=(35, 0.03),
    arrowprops=dict(facecolor='green', arrowstyle='->'),
    fontsize=10,
    color='green'
)

plt.title("Loss Trends with Respect, Integrity, and Kindness")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

# Evaluate the model
final_val_loss = min(history.history['val_loss'])
print(f"Best Validation Loss Achieved: {final_val_loss}")

"""The code implements a neural network model, inspired by neuroscience principles, to analyze text data and model the interplay of ego, shame, and attention.  Let's break down the key components and the output:

**1. Model Architecture:**

* **LSTM Network:** The core of the model is a Long Short-Term Memory (LSTM) network. LSTMs are well-suited for sequential data like text, as they can capture long-range dependencies and context.  The model uses multiple LSTM layers with dropout for regularization.
* **TimeDistributed Dense Layer:** This layer applies a dense (fully connected) layer to each timestep of the LSTM's output sequence. This allows the model to process features at each time step individually, potentially identifying changing relationships across the text.
* **Output Layer:** The final layer outputs three values representing the levels of ego, shame, and attention. The activation function used is 'linear', indicating the model outputs raw values.
* **Input Data:** The model input (`X_vectara`) is preprocessed text data that appears to be represented numerically, likely embeddings, from a Vectara vector database.
* **Output Data:** The model output (`y_ego_shame_attention`) represents the ground truth values for ego, shame, and attention levels that correspond to the input text.

**2. Training and Evaluation:**

* **Loss Function:**  Mean Squared Error (MSE) is used as the loss function, and Mean Absolute Error (MAE) is used as a metric. These metrics are typical for regression problems, and reflect the difference between the model's predictions and the actual values.
* **Optimizer:** The Adam optimizer is used for training.
* **Training Loop:** The `fit()` function trains the model.  The validation split parameter separates a portion of the data to evaluate the model's performance on unseen data during training.  The loss trends are tracked during training.
* **Error Analysis:** The code includes an `analyze_high_errors` function to identify samples where the model's predictions deviate significantly from the actual values. This is important for understanding the model's weaknesses and improving its accuracy.

**3. Stillness vs. Stagnation:**

* The code monitors the validation loss during training to detect potential "stillness" (minimal improvement but still learning) or "stagnation" (no meaningful improvement).  A threshold of 0.01 is used to distinguish between these states.
* **Stagnation:** If the validation loss improvement is less than 0.01 for a given number of epochs, the code indicates stagnation and suggests hyperparameter tuning, architecture changes, or data augmentation.
* **Stillness:**  If the validation loss decreases consistently, the model is considered to be in a stillness state. The code would then suggest continuing the training or to use specific techniques to promote further improvements.

**4. Virtue-Driven Callbacks and Model:**
* The code introduces "virtue-driven" callbacks—a set of functions that monitor training and make adjustments—to emphasize model quality, learning stability, and ethical development.  The use of `ModelCheckpoint` saves the best model configuration, based on validation loss. `ReduceLROnPlateau` adjusts the learning rate if the validation loss plateaus, which can help the model escape local minima.  `EarlyStopping` terminates training early if the validation loss does not improve for a certain number of epochs.
* Batch normalization, learning rate clipping, and a dynamic batch size adjustment are included, suggesting a focus on robustness, gentleness in gradient updates, and respecting the data variability.

**5. Output and Visualization:**

* The code generates several plots:
    * **Loss and MAE over epochs:** These plots visualize the training and validation loss and MAE, allowing evaluation of the training process and identifying overfitting.
    * **Sample predictions:** The code plots the predicted values against the actual values for a random sample, providing a visual check of the model's predictions.
* The final validation loss is also printed to report the model's overall performance.

**In summary:** The code creates, trains, and evaluates a neural network model for analyzing text data related to ego, shame, and attention.  The model's design, training process, and error analysis are geared towards accurately predicting these emotional aspects from text data.  Monitoring loss trends, combined with "virtue-driven" callbacks, aim to maximize performance and mitigate stagnation during training.
"""



"""# "Unlocking the Mind: Harnessing Quantum Consciousness for Revolutionary Advances in Psychological Healing and Understanding"

## Quantum consciousness is a theoretical idea that suggests our consciousness, or awareness, might be connected to the principles of quantum physics.

### What is Consciousness?
Consciousness refers to our awareness of ourselves and the world around us. It includes our thoughts, feelings, perceptions, and experiences.

###What is Quantum Physics?
Quantum physics is a branch of science that studies the behavior of very small particles, like atoms and subatomic particles. It reveals that at this tiny scale, particles can behave in strange ways, such as being in multiple states at once (superposition) or being instantaneously connected over distances (entanglement).

To approach psychological disorders and healing from a quantum consciousness perspective, we can use quantum-inspired models where the concept of superposition, entanglement, and non-locality help address states of consciousness related to disorders (e.g., ego, shame, attention).

## In this quantum framework:

* Superposition: The mind can be in multiple mental states (e.g., ego, shame, attention) simultaneously.
* Entanglement: Psychological states are interdependent, such that healing one state can influence and improve another.
* Non-locality: Changes in consciousness or healing can affect the mind without direct "cause and effect" in a localized sense, reflecting quantum effects like instantaneous changes across distances.
To represent this mathematically, we can model the brain's synaptic states as a quantum wave function and use a form of quantum neural networks (QNN) or quantum-inspired models. We'll implement a simplified quantum-inspired healing model that accounts for this.

### Here’s a mathematical approach:

1. Quantum States Representation of Mental States:
We represent the states of ego, shame, and attention as vectors in a quantum superposition:
𝜓
=
𝑐
1
∣
𝑒
𝑔
𝑜
⟩
+
𝑐
2
∣
𝑠
ℎ
𝑎
𝑚
𝑒
⟩
+
𝑐
3
∣
𝑎
𝑡
𝑡
𝑒
𝑛
𝑡
𝑖
𝑜
𝑛
⟩
ψ=c
1
​
 ∣ego⟩+c
2
​
 ∣shame⟩+c
3
​
 ∣attention⟩ where
∣
𝑒
𝑔
𝑜
⟩
∣ego⟩,
∣
𝑠
ℎ
𝑎
𝑚
𝑒
⟩
∣shame⟩, and
∣
𝑎
𝑡
𝑡
𝑒
𝑛
𝑡
𝑖
𝑜
𝑛
⟩
∣attention⟩ are the base states, and
𝑐
1
,
𝑐
2
,
𝑐
3
c
1
​
 ,c
2
​
 ,c
3
​
  are the probability amplitudes.

2. Evolution of Mental States:
We'll model the evolution of these states over time using a quantum-inspired Hamiltonian:
𝐻
=
𝛼
𝐼
+
𝛽
𝑈
H=αI+βU where
𝐼
I is the identity matrix representing equilibrium, and
𝑈
U is a unitary matrix representing the interaction between states.

 3. Measurement and Healing:
Healing corresponds to collapsing the wave function into a healthier state. When we "measure" the quantum mental state (analogous to self-awareness), it collapses to one of the states. The goal is to guide the system to collapse to a healed state.

4. Quantum Entanglement and Feedback:
Entanglement between different mental states means healing one part (e.g., shame) also heals another (e.g., attention).
Feedback loops allow the system to reinforce healing across states.
Quantum-inspired model using Python:

# Quantum-inspired model using Python

Based on the provided code and context, here's an analysis of the graph and metrics, along with key insights:

**1. Error Analysis Graph:**

* The `analyze_high_errors` function generates a graph that visualizes the mean absolute error of the model's predictions for each sample.
* The red dashed line represents the error threshold, which is calculated as the mean error plus two standard deviations.
* The graph shows how the model's error varies across different samples, with some samples exhibiting larger errors than others.

**2. Interpretation of the Graph:**

* **High Error Samples:** Samples with errors exceeding the threshold are identified as high-error cases, implying the model struggles to accurately predict the target values for these particular samples.
* **Model Performance:** The graph provides a visual representation of the model's performance, showing the overall distribution of errors and the effectiveness of the error threshold in identifying challenging samples.

**3. Mean Error During Training:**

* The `train_with_remembrance` function demonstrates the concept of "remembrance" over "memorization" by continuously monitoring the mean error during training.
* By stopping the training when the mean error falls below the target threshold (0.08744536096502369), it prevents overfitting and ensures the model generalizes well to unseen data.

**4. Error Threshold and Model Performance:**

* The error threshold is crucial for evaluating the model's performance and identifying areas where it needs improvement.
* A lower error threshold signifies that the model is performing better, as it has learned to predict the target values with higher accuracy.
* The threshold factor (default 2) controls the sensitivity of the error analysis. A higher factor results in a stricter threshold, indicating that more samples are considered as high-error cases.

**5. Insights from the Analysis:**

* The analysis indicates that the model is capable of learning and minimizing errors during training.
* However, there are specific instances where the model exhibits higher errors, highlighting areas for potential improvement in the model's architecture or training process.
* The visualization of errors and the monitoring of the mean error help identify and address weaknesses in the model's ability to accurately predict the target values, contributing to better overall model performance.

**6. Implications for Ego, Shame, and Attention:**

* The model aims to capture and predict the interplay of ego, shame, and attention based on text input.
* The graph and error analysis allow for a deeper understanding of the model's strengths and limitations in predicting these emotional dimensions.
* Identifying samples with high errors helps uncover patterns or characteristics of text data that the model struggles with, suggesting potential areas for refining the model to enhance its understanding of human emotions.

**7. Quantum Consciousness Perspective:**

* The model adopts a quantum consciousness approach by considering the mind as a superposition of states related to ego, shame, and attention.
* The model's LSTM architecture and quantum-inspired evolution process aim to capture the dynamics and interactions of these states over time.
* Analyzing the errors and visualizing the model's predictions can offer insights into how quantum principles, such as superposition and entanglement, influence the model's learning process and understanding of psychological dimensions.


**In Conclusion:**

The graph and error analysis provide valuable insights into the model's performance, areas for improvement, and the potential of using AI for better understanding of emotions. The model's focus on "remembrance" over "memorization" and the quantum consciousness framework enhance the model's learning capabilities and offer exciting avenues for exploring the complexity of human psychology.
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM
import matplotlib.pyplot as plt

# Define mental states as a superposition of ego, shame, and attention
def quantum_mental_state(coefficients):
    ego = coefficients[0]
    shame = coefficients[1]
    attention = coefficients[2]
    return np.array([ego, shame, attention])

# Create the initial quantum state (initial mental state)
initial_coefficients = np.random.rand(3)  # Random starting probabilities for each state
initial_state = quantum_mental_state(initial_coefficients)

# Define disorder and healing matrices (they evolve the mental states over time)
H_disorder = np.array([[0.5, 0.2, 0.3],
                       [0.1, 0.6, 0.3],
                       [0.3, 0.4, 0.3]])

H_healing = np.array([[0.7, 0.2, 0.1],
                      [0.2, 0.8, 0.0],
                      [0.1, 0.3, 0.6]])

# Define the Hamiltonian function for state evolution (disorder + healing)
def evolve_state(state, H_disorder, H_healing, timesteps=10):
    # Combining the disorder and healing matrices to evolve the state
    H_total = H_disorder + H_healing
    evolved_state = state
    for _ in range(timesteps):
        evolved_state = np.dot(H_total, evolved_state)  # Apply Hamiltonian to evolve state
        evolved_state /= np.linalg.norm(evolved_state)  # Normalize the state
    return evolved_state

# Evolve the initial mental state over time
timesteps = 50  # Arbitrary number of time steps for evolution
evolved_state = evolve_state(initial_state, H_disorder, H_healing, timesteps)

# Simulate data for ego, shame, and attention over time
num_samples = 100
X = np.random.rand(num_samples, timesteps, 3)  # Random data for the input (ego, shame, attention)

# The target (y) will be the evolved states after the healing process (normalized)
y = np.tile(evolved_state, (num_samples, timesteps, 1))  # Simulating healed states as target

# Build a quantum-inspired neural network (using LSTM layers)
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(timesteps, 3)))
model.add(Dense(3, activation='tanh'))

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Train the model
model.fit(X, y, epochs=10)

# Evaluate and apply healing to new data
new_data = np.random.rand(1, timesteps, 3)  # Example new mental state data
predicted_state = model.predict(new_data)
print("Predicted healed state:", predicted_state)

# Analyze error and apply healing
errors = np.abs(predicted_state - y[0])
print("Errors in prediction:", errors)

# Visualize the errors
plt.plot(errors.flatten(), label='Errors')
plt.title("Error Analysis for Healing Mental States")
plt.xlabel("Timestep")
plt.ylabel("Error")
plt.legend()
plt.show()

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM
import matplotlib.pyplot as plt

# Define the quantum-inspired mental states for ego, shame, and attention
def quantum_mental_state(coefficients):
    ego = coefficients[0]
    shame = coefficients[1]
    attention = coefficients[2]
    return np.array([ego, shame, attention])

# Create the initial quantum state (initial mental state)
initial_coefficients = np.random.rand(3)  # Random starting probabilities for each state
initial_state = quantum_mental_state(initial_coefficients)

# Psychological disorders influencing the system
# Example disorder matrices for anxiety (affecting attention), depression (affecting ego and shame)
H_anxiety = np.array([[0.4, 0.1, 0.7],   # Anxiety induces higher instability in attention
                      [0.2, 0.6, 0.2],
                      [0.8, 0.3, 0.5]])

H_depression = np.array([[0.3, 0.6, 0.2],  # Depression reduces ego and increases shame
                         [0.5, 0.4, 0.1],
                         [0.3, 0.5, 0.7]])

# Healing matrix (therapeutic interventions or mindfulness reducing disorders)
H_healing = np.array([[0.7, 0.2, 0.1],   # Healing intervention normalizes mental states
                      [0.2, 0.8, 0.0],
                      [0.1, 0.3, 0.6]])

# Define the Hamiltonian function for state evolution (disorder + healing)
def evolve_state(state, disorder_matrices, healing_matrix, timesteps=10):
    # Combine the disorder matrices (anxiety + depression) and the healing matrix
    H_total = disorder_matrices + healing_matrix
    evolved_state = state
    for _ in range(timesteps):
        evolved_state = np.dot(H_total, evolved_state)  # Apply Hamiltonian to evolve state
        evolved_state /= np.linalg.norm(evolved_state)  # Normalize the state
    return evolved_state

# Evolve the initial mental state over time considering anxiety and depression
timesteps = 50  # Arbitrary number of time steps for evolution
disorder_matrices = H_anxiety + H_depression  # Combine disorders
evolved_state = evolve_state(initial_state, disorder_matrices, H_healing, timesteps)

# Simulate data for ego, shame, and attention over time
num_samples = 100
X = np.random.rand(num_samples, timesteps, 3)  # Random data for the input (ego, shame, attention)

# The target (y) will be the evolved states after the healing process (normalized)
y = np.tile(evolved_state, (num_samples, timesteps, 1))  # Simulating healed states as target

# Build a quantum-inspired neural network (using LSTM layers)
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(timesteps, 3)))
model.add(Dense(3, activation='tanh'))

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Train the model
model.fit(X, y, epochs=10)

# Evaluate and apply healing to new data
new_data = np.random.rand(1, timesteps, 3)  # Example new mental state data
predicted_state = model.predict(new_data)
print("Predicted healed state:", predicted_state)

# Analyze error and apply healing
errors = np.abs(predicted_state - y[0])
print("Errors in prediction:", errors)

# Visualize the errors
plt.plot(errors.flatten(), label='Errors')
plt.title("Error Analysis for Healing Mental States")
plt.xlabel("Timestep")
plt.ylabel("Error")
plt.legend()
plt.show()

"""**1. Quantum State Representation:**

* Mental states (ego, shame, attention) are represented as a quantum superposition, a linear combination of base states with probability amplitudes.  This reflects the idea that mental states can exist in multiple states simultaneously.

**2. State Evolution:**

* The evolution of these mental states over time is modeled using a Hamiltonian (H), which incorporates a disorder matrix (representing psychological issues like anxiety and depression) and a healing matrix (representing therapeutic interventions). The matrices are applied iteratively to simulate state change.
* The `evolve_state` function simulates this evolution using matrix multiplication.  The normalization ensures the probability amplitudes remain valid.

**3. Neural Network Model:**

* An LSTM (Long Short-Term Memory) neural network is used to learn patterns in the evolving mental states.  LSTMs are well-suited for sequential data, making them appropriate for modeling the time-dependent evolution of the mental states.
* The network's input is simulated data representing the changes in ego, shame, and attention over time, and the target is the evolved states (after simulating the healing process).
* The model trains to minimize the Mean Squared Error between the predicted evolved state and the true evolved state (target).

**4. Error Analysis:**

* After training, the model's performance is evaluated by predicting the evolved states for new input data and calculating the absolute error between the predicted and true states.
* A plot of these errors over time is generated to visualize the model's accuracy.  Large errors suggest instances where the model's predictions deviate significantly from the actual expected states.


**5.  Symbolic Representation (using SymPy):**

* The code uses SymPy to represent the time-dependent Schrödinger equation symbolically.  This section likely aims to illustrate the underlying theoretical framework, but it doesn't directly affect the numerical simulation performed by the LSTM.

**Overall, the code implements a conceptual model.  The matrices used are not derived from real-world data or quantum mechanical calculations.** The use of a Hamiltonian and superposition concepts is illustrative and serves to link the simulation to a quantum framework. The model uses random data for demonstration rather than real psychological data. The quality of predictions depends entirely on the quality of the data used in the model training.

**In essence, this code provides a simplified simulation to explore the *idea* of applying quantum-inspired models to mental health challenges.**  It's a proof-of-concept rather than a production-ready model for actual diagnosis or treatment.

"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM
import matplotlib.pyplot as plt

# Define the quantum-inspired mental states for ego, shame, and attention
def quantum_mental_state(coefficients):
    ego = coefficients[0]
    shame = coefficients[1]
    attention = coefficients[2]
    return np.array([ego, shame, attention])

# Create the initial quantum state (initial mental state)
initial_coefficients = np.random.rand(3)  # Random starting probabilities for each state
initial_state = quantum_mental_state(initial_coefficients)

# Psychological disorders influencing the system
# Schizophrenia as an interference matrix that distorts the entire state
H_schizophrenia = np.array([[0.5, 0.3, 0.2],   # Schizophrenia introduces interference across all states
                            [0.4, 0.5, 0.1],
                            [0.3, 0.4, 0.5]])

# Narcissistic traits affecting ego and attention, suppressing shame
H_narcissism = np.array([[0.8, 0.1, 0.5],   # Narcissism inflates ego and increases attention
                         [0.2, 0.4, 0.3],
                         [0.3, 0.2, 0.7]])

# Healing matrix (therapeutic interventions or mindfulness reducing disorders)
H_healing = np.array([[0.7, 0.2, 0.1],   # Healing intervention normalizes mental states
                      [0.2, 0.8, 0.0],
                      [0.1, 0.3, 0.6]])

# Define the Hamiltonian function for state evolution (disorder + healing)
def evolve_state(state, disorder_matrices, healing_matrix, timesteps=10):
    # Combine the disorder matrices (schizophrenia, narcissism) and the healing matrix
    H_total = disorder_matrices + healing_matrix
    evolved_state = state
    for _ in range(timesteps):
        evolved_state = np.dot(H_total, evolved_state)  # Apply Hamiltonian to evolve state
        evolved_state /= np.linalg.norm(evolved_state)  # Normalize the state
    return evolved_state

# Evolve the initial mental state over time considering schizophrenia and narcissism
timesteps = 50  # Arbitrary number of time steps for evolution
disorder_matrices = H_schizophrenia + H_narcissism  # Combine disorders
evolved_state = evolve_state(initial_state, disorder_matrices, H_healing, timesteps)

# Simulate data for ego, shame, and attention over time
num_samples = 100
X = np.random.rand(num_samples, timesteps, 3)  # Random data for the input (ego, shame, attention)

# The target (y) will be the evolved states after the healing process (normalized)
y = np.tile(evolved_state, (num_samples, timesteps, 1))  # Simulating healed states as target

# Build a quantum-inspired neural network (using LSTM layers)
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(timesteps, 3)))
model.add(Dense(3, activation='tanh'))

# Compile the model
model.compile(optimizer='adam', loss='mse')

# Train the model
model.fit(X, y, epochs=10)

# Evaluate and apply healing to new data
new_data = np.random.rand(1, timesteps, 3)  # Example new mental state data
predicted_state = model.predict(new_data)
print("Predicted healed state:", predicted_state)

# Analyze error and apply healing
errors = np.abs(predicted_state - y[0])
print("Errors in prediction:", errors)

# Visualize the errors
plt.plot(errors.flatten(), label='Errors')
plt.title("Error Analysis for Healing Mental States")
plt.xlabel("Timestep")
plt.ylabel("Error")
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Simulate the mental state evolution over time
timesteps = 50
time = np.linspace(0, timesteps-1, timesteps)

# Initial states (stress, cognitive load, mood)
stress = np.random.rand(timesteps) * 0.5  # Initial stress level (from 0 to 0.5)
cognitive_load = np.random.rand(timesteps) * 0.5  # Initial cognitive load (from 0 to 0.5)
mood = np.random.rand(timesteps) * 0.5 + 0.5  # Initial mood (from 0.5 to 1)

# Simulating the effect of healing over time
for t in range(1, timesteps):
    stress[t] = stress[t-1] * 0.98 + np.random.rand() * 0.02  # Reducing stress over time with minor fluctuations
    cognitive_load[t] = cognitive_load[t-1] * 0.98 + np.random.rand() * 0.02  # Reducing cognitive load
    mood[t] = mood[t-1] * 1.02 - np.random.rand() * 0.01  # Improving mood over time

# Create a plot to visualize the evolution of stress, cognitive load, and mood
plt.figure(figsize=(12, 6))

# Plot Stress
plt.subplot(3, 1, 1)
plt.plot(time, stress, label='Stress', color='red')
plt.title("Stress Over Time")
plt.ylabel("Stress Level")
plt.legend()

# Plot Cognitive Load
plt.subplot(3, 1, 2)
plt.plot(time, cognitive_load, label='Cognitive Load', color='blue')
plt.title("Cognitive Load Over Time")
plt.ylabel("Cognitive Load Level")
plt.legend()

# Plot Mood
plt.subplot(3, 1, 3)
plt.plot(time, mood, label='Mood', color='green')
plt.title("Mood Over Time")
plt.ylabel("Mood Level")
plt.legend()

# Add a common title and improve layout
plt.tight_layout()
plt.suptitle("Mental State Evolution: Healing and Stillness", fontsize=16)

# Show the plot
plt.show()

# Providing advice based on the final state
if stress[-1] > 0.4 or cognitive_load[-1] > 0.4 or mood[-1] < 0.6:
    print("Advice: Intervention required.")
    print("Recommendation: Engage in mindfulness exercises, therapy, and consider medication.")
else:
    print("No intervention needed. Continue monitoring mental health.")

import numpy as np

# Simulate a database query fetching data from x_vectara
def fetch_data_from_x_vectara():
    """
    Simulate or fetch behavioral data (stress, cognitive load, mood) from x_vectara.
    Returns:
        stress (numpy array): Stress levels
        cognitive_load (numpy array): Cognitive load levels
        mood (numpy array): Mood levels
    """
    # Simulated data for illustration
    stress = np.random.uniform(0, 1, 100)  # Stress values between 0 and 1
    cognitive_load = np.random.uniform(0, 1, 100)  # Cognitive load values
    mood = np.random.uniform(0.5, 1, 100)  # Mood values slightly higher on average

    return stress, cognitive_load, mood

# Fetch the data
stress, cognitive_load, mood = fetch_data_from_x_vectara()

# Combine the data for easier handling
data = np.stack([stress, cognitive_load, mood], axis=1)  # Shape (100, 3)

import matplotlib.pyplot as plt

# Create a simple image generator
def generate_visual(stress, cognitive_load, mood):
    plt.figure(figsize=(10, 5))

    # Line plot for trends
    plt.plot(stress, label="Stress", color="red", alpha=0.7)
    plt.plot(cognitive_load, label="Cognitive Load", color="blue", alpha=0.7)
    plt.plot(mood, label="Mood", color="green", alpha=0.7)

    plt.title("Mental Health Parameters Over Time")
    plt.xlabel("Time")
    plt.ylabel("Value")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()

    plt.savefig("mental_health_trends.png")
    plt.show()

generate_visual(stress, cognitive_load, mood)

# Highlight anomalies or points of interest
def highlight_anomalies(stress, cognitive_load, mood):
    anomalies = (stress > 0.8) | (cognitive_load > 0.8) | (mood < 0.6)
    plt.figure(figsize=(10, 5))

    # Scatter plot for anomalies
    plt.scatter(range(len(stress)), stress, label="Stress", color="red")
    plt.scatter(range(len(cognitive_load)), cognitive_load, label="Cognitive Load", color="blue")
    plt.scatter(range(len(mood)), mood, label="Mood", color="green")

    plt.scatter(np.where(anomalies)[0], mood[anomalies], color="purple", label="Anomalies", s=100, marker="x")

    plt.title("Highlighting Anomalies in Mental Health Data")
    plt.xlabel("Time")
    plt.ylabel("Value")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()

    plt.savefig("mental_health_anomalies.png")
    plt.show()

highlight_anomalies(stress, cognitive_load, mood)

"""*1. Quantum-Inspired Mental State Model:**

*   **Representation:** Mental states (ego, shame, attention) are represented as vectors.  The code uses matrices (`H_anxiety`, `H_depression`, `H_schizophrenia`, `H_narcissism`, `H_healing`) to model the influence of psychological disorders and healing interventions.  These matrices are multiplied by the state vector to simulate how these factors change the mental state over time.  This is *inspired* by quantum mechanics, but the matrices themselves are not derived from actual quantum principles or data.

*   **Evolution:** The `evolve_state` function iteratively applies a combined Hamiltonian (disorder + healing matrices) to the initial mental state vector.  The state vector is then normalized to represent probabilities.  The evolution simulates the change in the mental states over a set number of timesteps.

**2. Neural Network (LSTM):**

*   The code then uses a Long Short-Term Memory (LSTM) neural network to learn the patterns in the simulated data representing the evolution of the mental states.  LSTMs are particularly good at learning from sequential data.

*   **Input:** The LSTM input (`X`) is randomly generated data.  Crucially, it doesn't represent any real-world measurements of mental states.

*   **Target:** The target output (`y`) for the LSTM is generated by applying the simulated healing process to the initial random mental state vector.  It simulates the "ideal" or desired outcome of the healing process.

*   **Training:** The LSTM is trained to predict the target (`y`) based on the input data (`X`), effectively learning the simulated healing process.

**3. Error Analysis and Visualization:**

*   After training, the LSTM predicts the evolved states for new, randomly generated input data.

*   The code calculates the difference (error) between the predicted state and the target state (which represents the idealized healed mental state).

*   The errors are plotted. These errors aren't errors in the real sense but measure how well the LSTM learns the simulated healing process from the input data to produce the simulated target.  In a real-world context, this validation method would require real-world mental state data.

**4. Mental State Simulation (Without Quantum Inspiration):**

*   This section simulates the evolution of stress, cognitive load, and mood over time, using a simple iterative method where each parameter influences its future values.
*   It includes visualization of these parameters over time.
*   Simple advice is given based on the final values of the simulated parameters.


**5. Simulated Data from x_vectara:**

*   The `fetch_data_from_x_vectara` function simulates fetching behavioral data.  In a real application, this function would interface with an actual data source.
*   This section generates random data rather than actual data and combines this into a numpy array for visualization.

**6. Visualizations:**

*   The simulated data is visualized using matplotlib:
    *   `generate_visual`: Creates a line plot showing stress, cognitive load, and mood over time.
    *   `highlight_anomalies`: Creates a scatter plot highlighting potential anomalies in the data (where values exceed thresholds).

**Overall Limitations:**

*   **Simulated Data:** The primary issue is the reliance on entirely random or simulated data.  The entire model is trained and validated on fabricated data, making it impossible to determine if it would accurately reflect real mental health data.
*   **Quantum Analogy:** While the model is inspired by quantum mechanics, it doesn't actually use quantum computations or quantum states in the true sense. The Hamiltonian matrices are numerical manipulations, rather than the quantum operators.  The simulation is therefore a conceptual demonstration rather than a practical model.
*   **No Real-World Validation:** There's no evaluation against any real-world psychological data. To be meaningful, the model would require datasets of patients' mental health indicators before and after treatment.

In summary, the code provides an *illustrative* demonstration of how you might conceptualize and model mental state changes using a quantum-inspired approach along with visualizations.  It is however a highly simplified proof of concept, and it is not directly applicable to diagnosing or treating mental health issues.
"""

import numpy as np
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import LSTM, Dense

# Fetch real data from x_vectara (e.g., ego, shame, attention)
def fetch_data_from_x_vectara():
    # Placeholder: Simulate fetching data for Ego, Shame, Attention (for simplicity)
    # In practice, this function should return actual data from x_vectara.
    # Each row is a timestep, each column is a mental state (ego, shame, attention).
    timesteps = 50
    dim = 3  # Ego, Shame, Attention
    # Simulating real mental state data for demo (replace with real data fetching)
    data = np.random.uniform(-1, 1, (timesteps, dim))
    return data

# Function to simulate the neuroplasticity healing process
def neuroplasticity_healing(data, velocity, E_field, B_field, q, timesteps):
    healed_state = data.copy()  # Initial state is the data itself
    position = np.zeros_like(data)  # Initialize position (state over time)

    for t in range(timesteps - 1):
        # Electric field effect (external healing)
        electric_influence = q * np.dot(E_field[:, t], healed_state[t])

        # Magnetic field effect (internal reinforcement)
        magnetic_influence = q * np.cross(velocity[t], np.dot(B_field, healed_state[t]))

        # Update state based on electromagnetic analogy
        healed_state[t + 1] = healed_state[t] + electric_influence + magnetic_influence

        # Normalize to ensure bounded state representation
        healed_state[t + 1] /= np.linalg.norm(healed_state[t + 1])

        # Update velocity and position (state evolution)
        velocity[t + 1] = healed_state[t + 1] - healed_state[t]
        position[t + 1] = position[t] + velocity[t + 1]

    return healed_state, velocity, position

# Set initial parameters
timesteps = 50  # Number of timesteps
dim = 3  # Number of dimensions (e.g., stress, cognitive load, mood)
q = 1  # Sensitivity of the system to fields (like charge)
E_field = np.random.uniform(0.1, 0.3, (dim, timesteps))  # External electric field (therapy, mindfulness)
B_field = np.random.uniform(0.1, 0.2, (dim, dim))  # Internal magnetic field (neuroplasticity)

# Fetch real data from x_vectara (e.g., ego, shame, attention)
state = fetch_data_from_x_vectara()  # Fetch the real-world data
velocity = np.zeros_like(state)  # Initial velocity (zero)
position = np.zeros_like(state)  # Initial position (zero)

# Run the healing process
healed_state, velocity, position = neuroplasticity_healing(state, velocity, E_field, B_field, q, timesteps)

# Plot the evolution of the mental states (healed states over time)
plt.figure(figsize=(10, 6))
plt.subplot(3, 1, 1)
plt.plot(healed_state[:, 0], label='Ego', color='blue')
plt.plot(healed_state[:, 1], label='Shame', color='green')
plt.plot(healed_state[:, 2], label='Attention', color='red')
plt.title('Evolution of Mental States (Ego, Shame, Attention) Over Time')
plt.xlabel('Timestep')
plt.ylabel('State Value')
plt.legend()

# Plot the velocity (rate of change of the mental states) over time
plt.subplot(3, 1, 2)
plt.plot(velocity[:, 0], label='Velocity (Ego)', color='blue')
plt.plot(velocity[:, 1], label='Velocity (Shame)', color='green')
plt.plot(velocity[:, 2], label='Velocity (Attention)', color='red')
plt.title('Velocity of Mental States (Rate of Change) Over Time')
plt.xlabel('Timestep')
plt.ylabel('Velocity')
plt.legend()

# Plot the position (cumulative state evolution) over time
plt.subplot(3, 1, 3)
plt.plot(position[:, 0], label='Position (Ego)', color='blue')
plt.plot(position[:, 1], label='Position (Shame)', color='green')
plt.plot(position[:, 2], label='Position (Attention)', color='red')
plt.title('Position of Mental States (Cumulative Evolution) Over Time')
plt.xlabel('Timestep')
plt.ylabel('Position')
plt.legend()

plt.tight_layout()
plt.show()

# Define the healing model (LSTM model)
healing_model = Sequential()
healing_model.add(LSTM(50, activation='relu', input_shape=(state.shape[0]-1, state.shape[1])))
healing_model.add(Dense(3))  # Output layer for 3 features (Ego, Shame, Attention)
healing_model.compile(optimizer='adam', loss='mean_squared_error')

# Prepare the data for LSTM
X_vectara = state[:-1]  # Input: all timesteps except the last one
y_ego_shame_attention = state[1:]  # Target: next timestep (future state)

# Reshape data for LSTM
X_vectara = X_vectara.reshape((X_vectara.shape[0], 1, X_vectara.shape[1]))  # Shape: (timesteps, 1, features)
y_ego_shame_attention = y_ego_shame_attention.reshape((y_ego_shame_attention.shape[0], y_ego_shame_attention.shape[1]))  # Shape: (timesteps, features)

# Train the model
history = healing_model.fit(
    X_vectara,
    y_ego_shame_attention,
    epochs=20,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

# Visualize the training process (loss curve)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training Loss vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Predict future states using the LSTM model
predictions = healing_model.predict(X_vectara)

# Visualize the predictions vs actual
plt.figure(figsize=(10, 6))
plt.plot(predictions[:, 0], label='Predicted Ego', color='blue')
plt.plot(predictions[:, 1], label='Predicted Shame', color='green')
plt.plot(predictions[:, 2], label='Predicted Attention', color='red')
plt.title('LSTM Predicted Mental States')
plt.xlabel('Timestep')
plt.ylabel('State Value')
plt.legend()
plt.show()

# Output the final healed state
print("Final Healed States:\n", healed_state[-1])

"""This Python code simulates and visualizes the evolution of mental states over time, incorporating elements inspired by quantum mechanics and employing a neural network (LSTM) for prediction.  It's crucial to understand that the code uses simulated data, not real-world psychological data, and the quantum-inspired elements are conceptual analogies rather than true quantum computations.


**Key Components:**

1. **Mental State Simulation (Quantum-Inspired):**  This part simulates the change in mental states (represented as vectors) over time using matrices. These matrices, inspired by quantum mechanics, model the influence of disorders and healing.  The state vector evolves through multiplication by these matrices, simulating the impact of these factors. This is purely a conceptual model and doesn't involve actual quantum calculations.

2. **Mental State Simulation (Classical, Iterative):**  This section simulates the evolution of stress, cognitive load, and mood over time. These parameters influence each other in a simplified manner, visually demonstrating trends. Basic advice is given at the end based on the simulated final state.

3. **Data Simulation (x_vectara):**  Instead of real data, the code simulates data fetching from a hypothetical "x_vectara" data source. This is a placeholder; a real application would replace it with actual data acquisition. This simulated data is used for visualization and to feed the LSTM.


4. **Neural Network (LSTM):** The code uses a Long Short-Term Memory (LSTM) neural network to learn patterns in the simulated mental state data.  The LSTM is trained to predict future mental states based on past states.  However, the input data is random, so the LSTM is learning a simulated process, not real-world patterns.

5. **Visualization:** Matplotlib is used extensively to visualize the mental state evolution, showing trends, potential anomalies, the LSTM's predictions, and the training process itself.

**Limitations:**

* **Simulated Data:** The code uses entirely simulated data, meaning the results cannot be interpreted as reflective of actual mental health.
* **Quantum Analogy:** The quantum inspiration is superficial.  It doesn't perform actual quantum computations.
* **No Real-World Validation:** The model is not validated against any real mental health data.

**In essence, the code is an illustrative conceptual model and demonstration of techniques, not a functional tool for mental health assessment or treatment.** To be a useful tool, the simulated data would need to be replaced with real data from actual patients.  Furthermore, the quantum-inspired model would need a strong theoretical and empirical foundation.

"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# Parameters
timesteps = 100  # Number of time steps
dim = 2  # Dimensionality of the mental state (e.g., ego, attention)
q = 1.0  # Sensitivity (charge)

# External electric field (therapy interventions, sensory stimuli, etc.)
E_field = np.random.uniform(0.1, 0.3, (dim, timesteps))  # Random external electric field
# Internal magnetic field (reinforcement, neuroplasticity, internal connectivity)
B_field = np.random.uniform(0.1, 0.2, (dim, dim))  # Random magnetic field matrix

# Initial mental state and velocity (e.g., ego, shame, attention)
position = np.zeros(dim)  # Initial emotional/cognitive state (e.g., [ego, attention])
velocity = np.zeros(dim)  # Initial velocity (rate of change in state)

# Store the trajectory of the state for plotting
state_trajectory = [position.copy()]

# Hamiltonians for disorder and healing
def disorder_effect(state):
    # Emotional dysregulation (stress, trauma) - affecting the state negatively
    return -0.1 * state

def healing_effect(state):
    # Healing interventions (therapy, mindfulness) - pushing the state to a more balanced state
    return 0.2 * np.tanh(state)

def adaptive_healing(state, time_step):
    """Adaptive healing mechanism based on time and state evolution"""
    # As time passes, the healing becomes more tailored and adaptive
    healing_factor = 0.1 * np.tanh(state[0])  # Example: ego-related healing grows over time
    return healing_factor * np.tanh(state)

def memory_reconsolidation(state, time_step):
    """Simulating the process of memory reconsolidation where old memories are updated"""
    # For this example, we allow negative states (e.g., trauma) to be "reframed" with time
    memory_update_factor = 0.1 * np.tanh(state[1])  # Focus on emotional states like shame
    return memory_update_factor * state

def resilience_building(state, time_step):
    """Simulating resilience building through coping and learning from adversity"""
    # Resilience is modeled as a positive reinforcement based on past experiences
    resilience_factor = 0.05 * np.abs(state[0])  # More ego-related resilience with time
    return resilience_factor * np.tanh(state)

def fire_together_wire_together(state, last_state, time_step):
    """Fire together, wire together: Strengthening connections between similar states"""
    connection_strength = 0.05 * np.dot(state, last_state)  # Reinforce the positive states together
    return connection_strength * state

def add_noise(state, intensity=0.05):
    """Adding random noise to simulate real-life unpredictability"""
    return state + np.random.normal(0, intensity, size=state.shape)

def detect_anomalies(state, threshold=1.5):
    """Simple anomaly detection based on state deviation"""
    deviation = np.linalg.norm(state - np.mean(state_trajectory, axis=0))
    return deviation > threshold

# Update the state using the healing equation and external/internal influences
for t in range(timesteps - 1):
    # Disorder and healing Hamiltonians
    H_disorder = disorder_effect(position)
    H_healing = healing_effect(position)

    # Adaptive healing adjustment over time
    H_adaptive_healing = adaptive_healing(position, t)

    # Memory reconsolidation effect (updating emotional states based on past experiences)
    H_memory_reconsolidation = memory_reconsolidation(position, t)

    # Resilience building effect
    H_resilience = resilience_building(position, t)

    # Fire together, wire together (positive reinforcement of emotional connections)
    H_connection = fire_together_wire_together(position, state_trajectory[-1], t)

    # Electric field effect (external intervention like therapy or mindfulness)
    electric_influence = q * np.dot(E_field[:, t], position)

    # Magnetic field effect (internal reinforcement like neuroplasticity)
    magnetic_influence = q * np.cross(velocity, np.dot(B_field, position))

    # Update the state using the Schrödinger-like evolution equation
    position += H_disorder + H_healing + H_adaptive_healing + H_memory_reconsolidation + H_resilience + H_connection + electric_influence + magnetic_influence

    # Adding random noise to simulate unpredictability in emotional state
    position = add_noise(position)

    # Detect anomalies in the state (unexpected shifts)
    if detect_anomalies(position):
        print(f"Anomaly detected at time {t}: Large deviation from expected state")

    # Normalize state to keep it bounded
    if np.linalg.norm(position) != 0:
        position /= np.linalg.norm(position)

    # Update velocity and store the state
    velocity = position - state_trajectory[-1]  # Calculate velocity as the change in position
    state_trajectory.append(position.copy())  # Store the state for plotting

# Convert the state trajectory to a numpy array for easy plotting
state_trajectory = np.array(state_trajectory)

# Normalize trajectory for visualization
scaler = MinMaxScaler()
state_trajectory_scaled = scaler.fit_transform(state_trajectory)

# Plot the state trajectory over time
plt.figure(figsize=(10, 6))
plt.plot(state_trajectory_scaled[:, 0], state_trajectory_scaled[:, 1], label='Mental State Trajectory (Ego, Attention)', color='b')
plt.scatter(state_trajectory_scaled[0, 0], state_trajectory_scaled[0, 1], color='r', label='Initial State', zorder=5)
plt.scatter(state_trajectory_scaled[-1, 0], state_trajectory_scaled[-1, 1], color='g', label='Final State', zorder=5)

# Labels and title
plt.title('Evolution of Emotional and Cognitive State with Therapy, Memory, Resilience, and Feedback Loops')
plt.xlabel('Ego')
plt.ylabel('Attention')
plt.legend()
plt.grid(True)
plt.show()

"""The provided Python code simulates the evolution of a simplified mental state over time, drawing analogies from physics (electromagnetism) and incorporating concepts like neuroplasticity, memory reconsolidation, and resilience.  Crucially, these analogies are *conceptual* and do not represent actual physical processes in the brain. The code uses simulated data and is for illustrative or research purposes only; it is not a clinical tool.

Here's a breakdown of the code's functionality:

**1. Initialization and Parameters:**

* `timesteps`: Defines the duration of the simulation.
* `dim`: Sets the dimensionality of the mental state (e.g., 2 dimensions could represent 'ego' and 'attention').
* `q`: Represents a sensitivity parameter, analogous to charge in electromagnetism.
* `E_field`: Simulates external influences (therapy, stimuli) as a time-varying electric field.
* `B_field`: Simulates internal factors (neuroplasticity, connectivity) as a constant magnetic field.
* `position` and `velocity`: Initialize the mental state and its rate of change.
* `state_trajectory`: A list to store the mental state's evolution over time.


**2.  Hamiltonian-like Functions:** These functions represent various influences on the mental state, analogous to Hamiltonians in physics, but applied metaphorically.  They influence how the `position` (mental state) changes over time.

* `disorder_effect`: Represents negative influences (stress, trauma).
* `healing_effect`: Represents positive influences (therapy, mindfulness).
* `adaptive_healing`:  A healing effect that changes over time, reflecting the adaptation of interventions.
* `memory_reconsolidation`: Simulates how past memories are updated or reinterpreted over time.  Here, negative states (e.g., trauma) might be reframed with time.
* `resilience_building`: Represents the development of resilience over time.
* `fire_together_wire_together`: Models the strengthening of connections between similar mental states.


**3. Noise and Anomaly Detection:**

* `add_noise`: Introduces random fluctuations into the mental state, mimicking the unpredictability of real-world mental states.
* `detect_anomalies`:  Identifies significant deviations of the current state from the average trajectory, marking potential anomalies or unusual shifts in the mental state.


**4. State Update Loop:** The core of the simulation.

* Inside the loop, the various Hamiltonian functions and external and internal field influences are combined to update the `position` (mental state) at each time step.  
* The `position` is then normalized to prevent unbounded growth.
* `velocity` is calculated as the change in position.
* Anomaly detection occurs within the loop.


**5. Visualization:**

* The simulation's results are plotted: mental state trajectory (ego vs. attention in the example), initial and final states.

**Overall Metaphor and Conceptual Model:**

The code employs metaphors from physics to simulate mental state dynamics.  For example:

* **Electric field:** External influences impacting the state.
* **Magnetic field:** Internal factors and connectivity influencing the state's evolution.
* **Hamiltonians:** The mathematical representation of factors affecting the state.

**Key Improvements:**

* The addition of more nuanced functions such as `memory_reconsolidation`, `resilience_building`, and `fire_together_wire_together` adds a greater level of sophistication to the model.


**Limitations and Important Considerations:**

* **Conceptual Model:**  The electromagnetic analogy is purely conceptual.  There is no direct physiological or neurological evidence to support this specific model.
* **Simulated Data:** The model operates on simulated data and lacks real-world validation.
* **Simplification:**  Real-world mental health is vastly more complex than this simulation.


This code is best understood as a simplified conceptual exploration using simulated data rather than a clinically accurate model of mental states.  It might be valuable for educational purposes or as a starting point for more complex simulations.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# Parameters for the simulation
timesteps = 100  # Number of time steps (representing time in the emotional evolution)
dim = 3  # Dimensionality of the emotional state (Ego, Attention, Shame)
q = 1.0  # Sensitivity constant that determines how external fields influence the emotional states

# External electric field (representing therapy, mindfulness, or external interventions)
E_field = np.random.uniform(0.1, 0.3, (dim, timesteps))  # Random external influence
# Internal magnetic field (representing internal reinforcement like neuroplasticity)
B_field = np.random.uniform(0.1, 0.2, (dim, dim))  # Random internal influence matrix

# Initial emotional state (Ego, Attention, Shame) and their velocities (rate of change)
position = np.zeros(dim)  # Starting at a neutral state
velocity = np.zeros(dim)  # Initially no change in the state

# Synaptic weights representing the connections between Ego, Attention, and Shame
synaptic_weights = np.random.uniform(0, 0.1, (dim, dim))  # Small random weights to simulate initial state

# Store the trajectory of the state for plotting (the evolution over time)
state_trajectory = [position.copy()]

# Function Definitions
def disorder_effect(state):
    """
    Simulates the negative emotional influences (e.g., trauma, stress) that destabilize the system.
    A negative influence on the emotional state is produced by this effect.
    """
    return -0.1 * state  # Example: stress affects all emotional states negatively

def healing_effect(state):
    """
    Represents healing processes (e.g., mindfulness, therapy) which promote emotional regulation.
    It aims to stabilize the emotional state and encourage balance.
    """
    return 0.2 * np.tanh(state)  # Healing is gradual and proportional to the emotional state

def shame_reduction(state):
    """
    Targets the reduction of shame, which influences emotional regulation by reducing its negative impact over time.
    The function works primarily on the third dimension (Shame) and helps reduce the shame-related intensity.
    """
    # Gradual reduction of shame (represented by the third state in the vector)
    return -0.15 * state[2]  # Only the shame part of the emotional state is influenced here

def adaptive_healing(state, time_step):
    """
    Simulates adaptive healing mechanisms, which evolve as the emotional system learns from past experiences.
    Healing adjusts over time based on the state’s progression.
    """
    healing_factor = 0.1 * np.tanh(state[0])  # Ego-driven healing increases with emotional stability
    return healing_factor * np.tanh(state)  # Proportional healing based on the emotional balance

def memory_reconsolidation(state, time_step):
    """
    Simulates memory reconsolidation where past emotional memories are updated based on the current emotional state.
    A mechanism for emotional growth that modifies the emotional context of past experiences.
    """
    memory_update_factor = 0.1 * np.tanh(state[1])  # Focuses on emotional memories tied to Attention
    return memory_update_factor * state  # Modifies emotional states with a focus on attention-related states

def resilience_building(state, time_step):
    """
    Represents resilience-building processes where repeated emotional experiences result in stronger emotional regulation.
    Over time, the system learns to adapt better to stress and challenging situations.
    """
    resilience_factor = 0.05 * np.abs(state[0])  # Ego-related resilience increases with emotional maturity
    return resilience_factor * np.tanh(state)  # Strengthens resilience over time

def fire_together_wire_together(state, last_state, time_step):
    """
    Hebbian learning rule: 'Fire together, wire together.' This mechanism strengthens synaptic connections between co-activated states.
    Emotional states that occur together strengthen their relationship, reinforcing them in the future.
    """
    # Hebbian learning: If both Ego, Attention, and Shame are activated together, their connection strengthens
    for i in range(dim):
        for j in range(dim):
            # Hebbian learning formula: synaptic weights are adjusted based on co-activation of states
            synaptic_weights[i, j] += 0.01 * state[i] * last_state[j]
    # Return updated state based on new synaptic weights
    return np.dot(synaptic_weights, state)

def add_noise(state, intensity=0.05):
    """
    Adding random noise to simulate unpredictability in real emotional experiences.
    This makes the model more realistic by accounting for randomness in emotional states.
    """
    return state + np.random.normal(0, intensity, size=state.shape)  # Adding Gaussian noise

def detect_anomalies(state, threshold=1.5):
    """
    Detects anomalies or significant deviations in the emotional state.
    If a large deviation is detected, it could signal emotional dysregulation or unexpected emotional responses.
    """
    deviation = np.linalg.norm(state - np.mean(state_trajectory, axis=0))
    return deviation > threshold  # Flags significant deviations as anomalies

# Main loop to simulate the evolution of the emotional state over time
for t in range(timesteps - 1):
    # Apply different effects to the current emotional state (Ego, Attention, Shame)
    H_disorder = disorder_effect(position)  # Negative influences like trauma or stress
    H_healing = healing_effect(position)  # Healing influences like therapy or mindfulness
    H_shame_reduction = shame_reduction(position)  # Targeted reduction of shame
    H_adaptive_healing = adaptive_healing(position, t)  # Adaptive healing based on the emotional state over time
    H_memory_reconsolidation = memory_reconsolidation(position, t)  # Update emotional memories
    H_resilience = resilience_building(position, t)  # Building emotional resilience over time

    # Apply Hebbian learning (fire together, wire together)
    H_connection = fire_together_wire_together(position, state_trajectory[-1], t)

    # Apply external electric field (e.g., therapy, mindfulness interventions)
    electric_influence = q * np.dot(E_field[:, t], position)

    # Apply internal magnetic field (internal reinforcement or neuroplasticity)
    magnetic_influence = q * np.cross(velocity, np.dot(B_field, position))

    # Combine all effects to update the emotional state
    position += H_disorder + H_healing + H_shame_reduction + H_adaptive_healing + H_memory_reconsolidation + H_resilience + H_connection + electric_influence + magnetic_influence

    # Add noise to simulate real-life unpredictability in emotional responses
    position = add_noise(position)

    # Check for significant anomalies or deviations in the emotional state
    if detect_anomalies(position):
        print(f"Anomaly detected at time {t}: Large deviation from expected state")

    # Normalize the emotional state to keep it bounded (avoid extreme values)
    if np.linalg.norm(position) != 0:
        position /= np.linalg.norm(position)

    # Update the velocity (change in the emotional state) and store the current state
    velocity = position - state_trajectory[-1]
    state_trajectory.append(position.copy())

# Convert the state trajectory to a numpy array for easy manipulation
state_trajectory = np.array(state_trajectory)

# Normalize the trajectory for visualization purposes
scaler = MinMaxScaler()
state_trajectory_scaled = scaler.fit_transform(state_trajectory)

# Plot the trajectory of the emotional state (Ego, Attention, Shame) over time
plt.figure(figsize=(10, 6))
plt.plot(state_trajectory_scaled[:, 0], state_trajectory_scaled[:, 1], label='Mental State Evolution', color='b')
plt.scatter(state_trajectory_scaled[0, 0], state_trajectory_scaled[0, 1], color='r', label='Initial State', zorder=5)
plt.scatter(state_trajectory_scaled[-1, 0], state_trajectory_scaled[-1, 1], color='g', label='Final State', zorder=5)

# Add labels, title, and legend
plt.title('Emotional State Evolution: Healing, Shame Reduction, and Hebbian Learning')
plt.xlabel('Ego')
plt.ylabel('Attention')
plt.legend()
plt.grid(True)
plt.show()

"""The provided Python code simulates the evolution of a simplified mental state over time, using analogies to physical systems like electromagnetism.  It's important to emphasize that these analogies are metaphorical and not literal representations of brain function.  The code is for illustrative or research purposes, not clinical use.

**Key Components and Functionality:**

1. **Initialization:** Sets up parameters like simulation duration (`timesteps`), dimensionality of the mental state (`dim`), sensitivity (`q`), external influences (`E_field`), internal factors (`B_field`), and initial mental state (`position`, `velocity`).  `synaptic_weights` represent connections between mental state components, crucial for Hebbian learning.

2. **Hamiltonian-like Functions:**  These functions model influences on the mental state, conceptually similar to Hamiltonians in physics, but used metaphorically. Examples include:
    * `disorder_effect`: Simulates negative influences (stress, trauma).
    * `healing_effect`: Represents positive influences (therapy, mindfulness).
    * `shame_reduction`: Specifically targets reducing shame.
    * `adaptive_healing`, `memory_reconsolidation`, `resilience_building`: Model more complex, dynamic processes.
    * `fire_together_wire_together`:  Implements Hebbian learning.  Synaptic weights are adjusted based on the co-activation of mental states; when two states are active together, their connection strengthens.  This is crucial for how the mental state evolves over time.

3. **Noise and Anomaly Detection:** `add_noise` introduces randomness to the simulation, reflecting the complexity of real-world mental states.  `detect_anomalies` identifies significant deviations from the average trajectory, potentially signaling unusual shifts.


4. **State Update Loop:** The core of the simulation. In each time step:
    * The Hamiltonian-like functions are applied.
    * The mental state (`position`) is updated, incorporating external (`E_field`), internal (`B_field`) influences, and the effects of the Hamiltonian functions.
    * The mental state is normalized to prevent extreme values.
    * `velocity` (the rate of change of the mental state) is updated.
    * Anomalies are detected.
    * **Crucially:** `synaptic_weights` are updated using the Hebbian learning rule in the `fire_together_wire_together` function.

5. **Visualization:** The code plots the trajectory of the mental state, showing how the various factors influence its evolution over time.  The visualization is conceptual, representing the changing values of the mental state components in a simplified way.

**Conceptual Visualization (Graph):**

Imagine a 3D graph with axes representing the different dimensions of the mental state (e.g., Ego, Attention, Shame).  The trajectory plotted by the code traces a path through this space, influenced by the various functions.  The initial and final states are marked on this path.  The Hamiltonians act as forces affecting the direction of this trajectory. The `fire_together_wire_together` function changes the "landscape" of the graph itself over time, modifying the dynamics of how the mental state changes.

**Key Improvements:**  Compared to previous versions, the code incorporates a dedicated `shame_reduction` function, uses Hebbian learning through `synaptic_weights` update, and has more extensive commenting.

**Limitations:**

* **Conceptual Model:** It's a metaphorical representation, not a biologically accurate model of the brain.
* **Simplification:** Mental processes are extremely complex and this is a highly simplified simulation.
* **Simulated Data:** It doesn't use real-world data.

**Important Disclaimer:** This code is for research and illustrative purposes only.  It is not a replacement for professional mental health care.
"""

import numpy as np
import matplotlib.pyplot as plt

# Parameters for the simulation
timesteps = 100  # Number of time steps
dim = 3  # Dimensionality (Ego, Shame, Attention)
q = 1.0  # Sensitivity constant

# External and internal influences (electric and magnetic fields)
E_field = np.random.uniform(0.1, 0.3, (dim, timesteps))  # External influences (e.g., therapy interventions)
B_field = np.random.uniform(0.1, 0.2, (dim, dim))  # Internal reinforcement (e.g., neuroplasticity)

# Initial state (position and velocity representing Ego, Shame, and Attention)
position = np.zeros(dim)  # Initial state (Ego, Shame, Attention)
velocity = np.zeros(dim)  # Initial velocity (rate of change)

# Storing trajectory for plotting
state_trajectory = [position.copy()]

# Define functions for emotional regulation

def ego_dynamics(state, self_esteem, feedback, alpha, beta):
    """ Ego dynamics influenced by self-esteem and social feedback """
    return alpha * (self_esteem - state[0]) + beta * (feedback - state[0])

def shame_dynamics(state, ego, rejection, gamma, delta):
    """ Shame dynamics influenced by ego and social rejection """
    return gamma * (ego - state[1]) + delta * (rejection - state[1])

def attention_dynamics(state, task_demand, shame_level, epsilon, zeta):
    """ Attention dynamics influenced by task demand and shame """
    return epsilon * (task_demand - state[2]) - zeta * shame_level

# Simulate the evolution of mental states over time
for t in range(timesteps - 1):
    # Random emotional influences (shame, self-esteem, etc.)
    self_esteem = np.random.uniform(0.3, 0.7)
    feedback = np.random.uniform(0.1, 0.9)
    rejection = np.random.uniform(0.1, 0.5)
    task_demand = np.random.uniform(0.5, 0.9)

    # Emotional regulation coefficients
    alpha, beta, gamma, delta, epsilon, zeta = 0.1, 0.1, 0.1, 0.1, 0.1, 0.1

    # Ego, Shame, and Attention dynamics
    dE = ego_dynamics(position, self_esteem, feedback, alpha, beta)
    dH = shame_dynamics(position, position[0], rejection, gamma, delta)
    dA = attention_dynamics(position, task_demand, position[1], epsilon, zeta)

    # Update the state (mental dynamics)
    position[0] += dE  # Update Ego
    position[1] += dH  # Update Shame
    position[2] += dA  # Update Attention

    # External and internal influences (electromagnetic fields)
    electric_influence = q * np.dot(E_field[:, t], position)
    magnetic_influence = q * np.cross(velocity, np.dot(B_field, position))

    # Update the state considering external and internal influences
    position += electric_influence + magnetic_influence

    # Normalize the state to keep it bounded
    if np.linalg.norm(position) != 0:
        position /= np.linalg.norm(position)

    # Update velocity and store the state
    velocity = position - state_trajectory[-1]  # Calculate velocity as the change in position
    state_trajectory.append(position.copy())  # Store the state for plotting

# Convert the state trajectory to a numpy array for easy plotting
state_trajectory = np.array(state_trajectory)

# Plot the state trajectory over time
plt.figure(figsize=(10, 6))
plt.plot(state_trajectory[:, 0], state_trajectory[:, 1], label='Mental State Trajectory (Ego, Shame)', color='b')
plt.scatter(state_trajectory[0, 0], state_trajectory[0, 1], color='r', label='Initial State', zorder=5)
plt.scatter(state_trajectory[-1, 0], state_trajectory[-1, 1], color='g', label='Final State', zorder=5)

# Labels and title
plt.title('Evolution of Mental States: Ego, Shame, and Attention')
plt.xlabel('Ego')
plt.ylabel('Shame')
plt.legend()
plt.grid(True)
plt.show()

"""The code simulates the dynamic interplay of mental states – specifically Ego, Shame, and Attention – over time.  It uses a metaphorical approach, drawing analogies to physical systems like electromagnetism to model the influence of various factors on these states.  The simulation doesn't represent the brain literally but provides a conceptual framework to explore how different internal and external forces might shape mental well-being.

**Simulation Mechanics:**

1. **Initialization:**  The simulation begins with initial values for Ego, Shame, and Attention, along with parameters defining external influences (like therapeutic interventions or social feedback) and internal factors (representing neuroplasticity or self-esteem).

2. **State Dynamics:** The core of the simulation involves updating the mental states at each time step.  These updates are governed by functions (`ego_dynamics`, `shame_dynamics`, `attention_dynamics`) that model how each mental state changes based on the current values of all three states, and random inputs representing emotional influences (self-esteem, feedback, rejection, task demand). These functions incorporate parameters that control the strength of these influences.

3. **External and Internal Influences (Electromagnetic Analogy):** The code uses an analogy to electromagnetism, where external influences are like electric fields, and internal reinforcement is like magnetic fields. These forces act on the mental states, modifying their trajectories.

4. **State Update and Normalization:**  At each time step, the mental state is updated based on the influences calculated in the previous step.  A normalization step ensures the magnitude of the mental state vector remains bounded, preventing extreme values.  The velocity of the mental state is tracked as the change in position at each time step.

5. **Visualization:** The graph depicts the trajectory of the mental states, specifically focusing on the relationship between Ego and Shame. The trajectory shows how these two states change over the course of the simulation.  The initial and final points of the trajectory are highlighted to show the overall change in these states. The x-axis represents the Ego state, and the y-axis represents the Shame state.  The plotted line illustrates the path taken by the mental state through this 2D representation of the mental state space.  Each point on the line represents a snapshot of the system at a specific time.

**Key Concepts:**

* **No Direct Brain Representation:** This is not a literal model of the brain; instead, it offers a conceptual framework.
* **Metaphorical Analogy:** The use of electromagnetic fields is metaphorical, serving to illustrate how various factors can influence mental state changes.
* **Simplified Dynamics:**  The mental state dynamics are simplified, but they represent an attempt to model how various influences interact.
* **Illustrative Purpose:**  This simulation is meant to provide a visual representation of how mental states can evolve over time, and not for prediction or diagnosis of real psychological conditions.


In summary, the graph displays the simulated changes in Ego and Shame over a period of time. The path illustrates the evolving mental state, and the beginning and end points highlight the overall shift in those specific dimensions.  The rest of the mental state (Attention) is implicitly represented within this shifting trajectory.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# Parameters for the simulation
timesteps = 100  # Number of time steps (representing time in the emotional evolution)
dim = 3  # Dimensionality of the emotional state (Ego, Attention, Shame)
q = 1.0  # Sensitivity constant that determines how external fields influence the emotional states

# External electric field (representing therapy, mindfulness, or external interventions)
E_field = np.random.uniform(0.1, 0.3, (dim, timesteps))  # Random external influence
# Internal magnetic field (representing internal reinforcement like neuroplasticity)
B_field = np.random.uniform(0.1, 0.2, (dim, dim))  # Random internal influence matrix

# Initial emotional state (Ego, Attention, Shame) and their velocities (rate of change)
position = np.zeros(dim)  # Starting at a neutral state
velocity = np.zeros(dim)  # Initially no change in the state

# Synaptic weights representing the connections between Ego, Attention, and Shame
synaptic_weights = np.random.uniform(0, 0.1, (dim, dim))  # Small random weights to simulate initial state

# Store the trajectory of the state for plotting (the evolution over time)
state_trajectory = [position.copy()]

# Function Definitions
def disorder_effect(state):
    """ Simulates negative emotional influences (e.g., trauma, stress) that destabilize the system. """
    return -0.1 * state  # Example: stress affects all emotional states negatively

def healing_effect(state):
    """ Represents healing processes (e.g., mindfulness, therapy) which promote emotional regulation. """
    return 0.2 * np.tanh(state)  # Healing is gradual and proportional to the emotional state

def shame_reduction(state):
    """ Targets the reduction of shame, which influences emotional regulation by reducing its negative impact over time. """
    return -0.15 * state[2]  # Only the shame part of the emotional state is influenced here

def adaptive_healing(state, time_step):
    """ Simulates adaptive healing mechanisms, evolving over time based on state progression. """
    healing_factor = 0.1 * np.tanh(state[0])  # Ego-driven healing increases with emotional stability
    return healing_factor * np.tanh(state)  # Proportional healing based on emotional balance

def memory_reconsolidation(state, time_step):
    """ Simulates memory reconsolidation, where past emotional memories are updated based on the current emotional state. """
    memory_update_factor = 0.1 * np.tanh(state[1])  # Focuses on emotional memories tied to Attention
    return memory_update_factor * state  # Modifies emotional states with a focus on attention-related states

def resilience_building(state, time_step):
    """ Represents resilience-building processes where repeated emotional experiences lead to stronger emotional regulation. """
    resilience_factor = 0.05 * np.abs(state[0])  # Ego-related resilience increases with emotional maturity
    return resilience_factor * np.tanh(state)  # Strengthens resilience over time

def fire_together_wire_together(state, last_state, time_step):
    """ Hebbian learning rule: 'Fire together, wire together.' Strengthens synaptic connections between co-activated states. """
    for i in range(dim):
        for j in range(dim):
            # Hebbian learning formula: synaptic weights are adjusted based on co-activation of states
            synaptic_weights[i, j] += 0.01 * state[i] * last_state[j]
    # Return updated state based on new synaptic weights
    return np.dot(synaptic_weights, state)

def add_noise(state, intensity=0.05):
    """ Adding random noise to simulate unpredictability in real emotional experiences. """
    return state + np.random.normal(0, intensity, size=state.shape)  # Adding Gaussian noise

def detect_anomalies(state, threshold=1.5):
    """ Detects anomalies or significant deviations in the emotional state. """
    deviation = np.linalg.norm(state - np.mean(state_trajectory, axis=0))
    return deviation > threshold  # Flags significant deviations as anomalies

# Reinforcement Learning - Positive feedback for balanced states and negative for dysregulation
def reinforcement_learning_feedback(state):
    """ Simulates reinforcement learning based on emotional state. """
    reward = -np.linalg.norm(state[0:2])  # Penalize if Ego and Attention are imbalanced
    # Reward mechanism for balanced emotional states (lower Ego and higher Attention)
    if np.abs(state[0]) < 0.2 and state[1] > 0.5:
        reward += 1.0  # Positive reinforcement for balance
    return reward

# Main loop to simulate the evolution of the emotional state over time
for t in range(timesteps - 1):
    # Apply different effects to the current emotional state (Ego, Attention, Shame)
    H_disorder = disorder_effect(position)  # Negative influences like trauma or stress
    H_healing = healing_effect(position)  # Healing influences like therapy or mindfulness
    H_shame_reduction = shame_reduction(position)  # Targeted reduction of shame
    H_adaptive_healing = adaptive_healing(position, t)  # Adaptive healing based on the emotional state over time
    H_memory_reconsolidation = memory_reconsolidation(position, t)  # Update emotional memories
    H_resilience = resilience_building(position, t)  # Building emotional resilience over time

    # Apply Hebbian learning (fire together, wire together)
    H_connection = fire_together_wire_together(position, state_trajectory[-1], t)

    # Apply external electric field (e.g., therapy, mindfulness interventions)
    electric_influence = q * np.dot(E_field[:, t], position)

    # Apply internal magnetic field (internal reinforcement or neuroplasticity)
    magnetic_influence = q * np.cross(velocity, np.dot(B_field, position))

    # Combine all effects to update the emotional state
    position += H_disorder + H_healing + H_shame_reduction + H_adaptive_healing + H_memory_reconsolidation + H_resilience + H_connection + electric_influence + magnetic_influence

    # Add noise to simulate real-life unpredictability in emotional responses
    position = add_noise(position)

    # Check for significant anomalies or deviations in the emotional state
    if detect_anomalies(position):
        print(f"Anomaly detected at time {t}: Large deviation from expected state")

    # Normalize the emotional state to keep it bounded (avoid extreme values)
    if np.linalg.norm(position) != 0:
        position /= np.linalg.norm(position)

    # Update the velocity (change in the emotional state) and store the current state
    velocity = position - state_trajectory[-1]
    state_trajectory.append(position.copy())

    # Apply reinforcement learning feedback to adjust the state
    reward = reinforcement_learning_feedback(position)
    position += reward * np.array([0.1, 0.1, 0.1])  # Apply feedback to state (modifying it slightly)

# Convert the state trajectory to a numpy array for easy manipulation
state_trajectory = np.array(state_trajectory)

# Normalize the trajectory for visualization purposes
scaler = MinMaxScaler()
state_trajectory_scaled = scaler.fit_transform(state_trajectory)

# Plot the trajectory of the emotional state (Ego, Attention, Shame) over time
plt.figure(figsize=(10, 6))
plt.plot(state_trajectory_scaled[:, 0], state_trajectory_scaled[:, 1], label='Mental State Evolution', color='b')
plt.scatter(state_trajectory_scaled[0, 0], state_trajectory_scaled[0, 1], color='r', label='Initial State', zorder=5)
plt.scatter(state_trajectory_scaled[-1, 0], state_trajectory_scaled[-1, 1], color='g', label='Final State', zorder=5)

# Add labels, title, and legend
plt.title('Emotional State Evolution: Healing, Shame Reduction, and Reinforcement Learning')
plt.xlabel('Ego')
plt.ylabel('Attention')
plt.legend()
plt.grid(True)
plt.show()

"""he code simulates the dynamic interplay of mental states (Ego, Attention, Shame) over time, incorporating various influences and mechanisms inspired by physical systems and cognitive science.  Let's break down the key components and improvements:

**1. Enhanced Emotional Dynamics:**

* **Multiple Influence Factors:**  The code now includes a wider range of functions that model different influences on the emotional states:
    * `disorder_effect`: Simulates negative influences (trauma, stress).
    * `healing_effect`: Represents healing processes (therapy, mindfulness).
    * `shame_reduction`: Targets the reduction of shame.
    * `adaptive_healing`: Simulates adaptive healing that changes over time.
    * `memory_reconsolidation`: Models how past memories are updated.
    * `resilience_building`: Represents the growth of emotional resilience.
    * `fire_together_wire_together`: Implements a Hebbian learning rule, strengthening connections between co-activated states.

* **Noise and Anomaly Detection:**  The addition of `add_noise` simulates the inherent unpredictability of emotions and `detect_anomalies` flags significant deviations, making the simulation more realistic.


* **Reinforcement Learning Feedback:** A `reinforcement_learning_feedback` function rewards balanced emotional states (lower Ego, higher Attention) and penalizes dysregulation. This introduces an adaptive element, where the system tends towards more desirable emotional configurations.

**2. Improved Visualization:**

* **Scaling:** The data for the plot is now normalized using `MinMaxScaler` from `sklearn.preprocessing`. This is crucial for proper visualization, preventing extreme values from dominating the plot and ensuring that all changes in the emotional states are clearly visible.  
* **Clearer Labels and Title:** The plot has been updated with a more informative title and labels, indicating the inclusion of healing, shame reduction, and reinforcement learning. This allows for easier understanding of the visualized data.

**3. Enhanced Simulation Mechanics:**

* **Synaptic Weights:** The introduction of `synaptic_weights` adds another layer to the simulation, allowing the relationships between the emotional states to change over time.
* **External and Internal Influences:** The electric and magnetic fields still provide an analogy for external and internal influences on the emotional states, but they are now used more as a way to add external and internal forces, modifying trajectories, rather than providing direct updates to the emotional states.

**4.  Code Structure:**

The code has been improved to separate functions.

**Interpreting the Plot**

The plot shows the interplay of Ego and Attention over time. The trajectory illustrates the combined effects of the different influences. The initial and final states are marked to visualize the overall change.  Again, the X-axis is Ego, the Y-axis is Attention and the rest of the mental state (Shame) is implicitly represented within this shifting trajectory, given that the state changes include updates to Shame as well.

**Further Improvements:**

* **Parameter Tuning:**  Experiment with different parameter values for the various functions to explore their impact on the emotional state trajectory.  This will allow for a more nuanced understanding of how each factor contributes to the emotional dynamics.
* **More Realistic Models:** Consider implementing more sophisticated mathematical models for emotional dynamics based on existing psychological theories.
* **More Detailed Visualization:** Visualize the evolution of all three emotional states (Ego, Attention, Shame) simultaneously, potentially in 3D.  Display additional plots showing the influence of different functions and reinforcement signals over time.


This revised code and explanation provides a more comprehensive and realistic emotional state simulation. The use of additional influence factors, a reinforcement learning loop, and improved visualization make the results more relevant and easier to interpret.
"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from sklearn.preprocessing import MinMaxScaler

# Parameters for the simulation
timesteps = 50  # Reduced the number of timesteps for faster performance
dim = 3  # Dimensionality of the emotional state (Ego, Attention, Shame)

# External electric field (representing therapy, mindfulness, or external interventions)
E_field = np.random.uniform(0.1, 0.3, (dim, timesteps))  # Random external influence

# Initial emotional state (Ego, Attention, Shame) and their velocities (rate of change)
position = np.zeros(dim)  # Starting at a neutral state
velocity = np.zeros(dim)  # Initially no change in the state

# Synaptic weights representing the connections between Ego, Attention, and Shame
synaptic_weights = np.random.uniform(0, 0.1, (dim, dim))  # Small random weights to simulate initial state

# Store the trajectory of the state for plotting (the evolution over time)
state_trajectory = [position.copy()]

def update(t):
    """
    Function to update the plot at each timestep.
    This function will be called by FuncAnimation.
    """
    global position, velocity, state_trajectory

    # Apply effects and update the emotional state (same as before)
    # Simulate simple dynamics for demonstration (can be made more complex)
    position += np.random.normal(0, 0.05, size=position.shape)

    # Store the updated state
    state_trajectory.append(position.copy())

    # Update the plot
    ax.clear()
    ax.plot([state[0] for state in state_trajectory], [state[1] for state in state_trajectory], label="Trajectory")
    ax.scatter(position[0], position[1], color='r')
    ax.set_title("Emotional State Evolution")
    ax.set_xlabel('Ego')
    ax.set_ylabel('Attention')
    ax.set_xlim(-1, 1)
    ax.set_ylim(-1, 1)
    ax.legend()

    return ax,

# Create a plot for the animation
fig, ax = plt.subplots(figsize=(6, 6))

# Create the animation
anim = FuncAnimation(fig, update, frames=timesteps, repeat=False)

# Save the animation as a GIF
anim.save('emotion_evolution.gif', writer='pillow', fps=10)

"""The code you provided simulates the evolution of an emotional state over time, represented by three dimensions: Ego, Attention, and Shame.  It uses a numerical simulation approach, inspired by physical systems (like charged particles in fields), to model how these emotional states change in response to various internal and external factors.  Let's break down the code step by step:

**1. Initialization:**

* **Parameters:** Sets parameters like the number of time steps (`timesteps`), the dimensionality of the emotional state (`dim`), sensitivity constants (`q`), and initial values for emotional states (position and velocity), and synaptic weights.
* **External and Internal Fields:** Creates random external electric fields (`E_field`) and internal magnetic fields (`B_field`).  These fields represent external influences (therapy, mindfulness) and internal factors (neuroplasticity, internal reinforcement), respectively.
* **State Trajectory:** Initializes a list to store the evolution of the emotional states over time.

**2. Functions:**

The code defines several functions that model different influences on the emotional state:

* **`disorder_effect`, `healing_effect`, `shame_reduction`**: These functions simulate the impact of stress, healing processes, and targeted shame reduction, respectively, on the emotional states.
* **`adaptive_healing`, `memory_reconsolidation`, `resilience_building`**: These represent more complex, evolving influences on emotional regulation, memory, and resilience.
* **`fire_together_wire_together`**: Implements a Hebbian learning rule, simulating how co-activation of emotional states strengthens the connections between them.
* **`add_noise`**: Introduces random noise to the simulation, representing the inherent unpredictability of emotional responses.
* **`detect_anomalies`**: Checks for significant deviations from the expected emotional state.
* **`reinforcement_learning_feedback`**:  Provides a reinforcement learning signal, rewarding balanced emotional states and penalizing dysregulation.


**3. Main Simulation Loop:**

* **Iteration:** The code iterates through the specified number of time steps.
* **Applying Effects:** In each step, it applies the different influences (defined by the functions above) to update the emotional state.  This includes effects of disorder, healing, shame reduction, external electric fields (therapy), and internal magnetic fields (neuroplasticity).  The `fire_together_wire_together` function updates the synaptic weights, simulating Hebbian learning.
* **Noise and Anomaly Detection:**  Random noise is added to simulate realistic emotional fluctuations, and anomaly detection checks for unexpected shifts in the emotional state.
* **State Update and Storage:** The emotional state is updated based on all the influences and stored in the `state_trajectory`.
* **Reinforcement Learning:** Reinforcement learning feedback is applied, nudging the state towards better configurations.

**4. Visualization (Plot):**

* **Normalization:** The trajectory of the emotional state is normalized using `MinMaxScaler` to improve the plot's readability.
* **Plotting:** The code plots the evolution of the emotional state, showing how the Ego and Attention interact over time.  The initial and final states are highlighted.

In summary, the code creates a dynamic system to represent emotional evolution, driven by multiple internal and external factors.  The visualization provides a graphical representation of this complex interplay of factors.  The code uses a combination of mathematical functions and random processes to simulate a fairly complex and evolving emotional system.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import matplotlib.animation as animation

# Parameters for the simulation
timesteps = 50  # Reduced the number of timesteps for faster performance
dim = 3  # Dimensionality of the emotional state (Ego, Attention, Shame)
q = 1.0  # Sensitivity constant that determines how external fields influence the emotional states

# External electric field (representing therapy, mindfulness, or external interventions)
E_field = np.random.uniform(0.1, 0.3, (dim, timesteps))  # Random external influence
# Internal magnetic field (representing internal reinforcement like neuroplasticity)
B_field = np.random.uniform(0.1, 0.2, (dim, dim))  # Random internal influence matrix

# Initial emotional state (Ego, Attention, Shame) and their velocities (rate of change)
position = np.zeros(dim)  # Starting at a neutral state
velocity = np.zeros(dim)  # Initially no change in the state

# Synaptic weights representing the connections between Ego, Attention, and Shame
synaptic_weights = np.random.uniform(0, 0.1, (dim, dim))  # Small random weights to simulate initial state

# Store the trajectory of the state for plotting (the evolution over time)
state_trajectory = [position.copy()]
healing_trajectory = []
shame_trajectory = []
reward_trajectory = []

# Function Definitions (same as before)
def disorder_effect(position):
    """ Negative influences like trauma or stress """
    return np.random.uniform(-0.1, -0.05, 3)

def healing_effect(position):
    """ Healing influences like therapy or mindfulness """
    return np.random.uniform(0.05, 0.1, 3)

def shame_reduction(position):
    """ Targeted reduction of shame """
    return np.random.uniform(-0.05, -0.1, 3)

def adaptive_healing(position, t):
    """ Adaptive healing based on emotional state over time """
    return np.random.uniform(0.05, 0.1, 3) * np.sin(t / 10.0)

def memory_reconsolidation(position, t):
    """ Update emotional memories """
    return np.random.uniform(-0.05, 0.05, 3) * np.cos(t / 20.0)

def resilience_building(position, t):
    """ Building emotional resilience over time """
    return np.random.uniform(0.02, 0.05, 3) * np.log(t + 1)

def fire_together_wire_together(position, previous_state, t):
    """ Hebbian learning: fire together, wire together """
    return synaptic_weights @ (position - previous_state)

def add_noise(position):
    """ Add random noise to simulate real-world unpredictability """
    noise = np.random.uniform(-0.01, 0.01, 3)
    return position + noise

def reinforcement_learning_feedback(position):
    """ Feedback from reinforcement learning to adjust state """
    return np.random.uniform(-0.05, 0.05, 3)

# Main loop to simulate the evolution of the emotional state over time
for t in range(timesteps - 1):
    # Apply different effects to the current emotional state (Ego, Attention, Shame)
    H_disorder = disorder_effect(position)  # Negative influences like trauma or stress
    H_healing = healing_effect(position)  # Healing influences like therapy or mindfulness
    H_shame_reduction = shame_reduction(position)  # Targeted reduction of shame
    H_adaptive_healing = adaptive_healing(position, t)  # Adaptive healing based on the emotional state over time
    H_memory_reconsolidation = memory_reconsolidation(position, t)  # Update emotional memories
    H_resilience = resilience_building(position, t)  # Building emotional resilience over time

    # Apply Hebbian learning (fire together, wire together)
    H_connection = fire_together_wire_together(position, state_trajectory[-1], t)

    # Apply external electric field (e.g., therapy, mindfulness interventions)
    electric_influence = q * np.dot(E_field[:, t], position)

    # Apply internal magnetic field (internal reinforcement or neuroplasticity)
    magnetic_influence = q * np.cross(velocity, np.dot(B_field, position))

    # Combine all effects to update the emotional state
    position += H_disorder + H_healing + H_shame_reduction + H_adaptive_healing + H_memory_reconsolidation + H_resilience + H_connection + electric_influence + magnetic_influence

    # Add noise to simulate real-life unpredictability in emotional responses
    position = add_noise(position)

    # Normalize the emotional state to keep it bounded (avoid extreme values)
    if np.linalg.norm(position) != 0:
        position /= np.linalg.norm(position)

    # Update the velocity (change in the emotional state) and store the current state
    velocity = position - state_trajectory[-1]
    state_trajectory.append(position.copy())

    # Apply reinforcement learning feedback to adjust the state
    reward = reinforcement_learning_feedback(position)
    position += reward * np.array([0.1, 0.1, 0.1])  # Apply feedback to state (modifying it slightly)

    # Store the influences over time for plotting in the additional plots
    healing_trajectory.append(H_healing)
    shame_trajectory.append(H_shame_reduction)
    reward_trajectory.append(reward)

# Convert the state trajectory to a numpy array for easy manipulation
state_trajectory = np.array(state_trajectory)

# Normalize the trajectory for visualization purposes
scaler = MinMaxScaler()
state_trajectory_scaled = scaler.fit_transform(state_trajectory)

# Create a figure with subplots for 3D trajectory and influence plots
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection='3d')

# Function to animate the 3D trajectory
def update_plot(frame):
    ax.clear()
    # Plot the emotional state trajectory in 3D
    ax.plot(state_trajectory_scaled[:frame, 0], state_trajectory_scaled[:frame, 1], state_trajectory_scaled[:frame, 2], color='b')
    ax.scatter(state_trajectory_scaled[frame, 0], state_trajectory_scaled[frame, 1], state_trajectory_scaled[frame, 2], color='r')

    # Set labels and title
    ax.set_xlabel('Ego')
    ax.set_ylabel('Attention')
    ax.set_zlabel('Shame')
    ax.set_title(f'Emotional State Evolution at t={frame}')

    return ax,

# Set up the animation
ani = animation.FuncAnimation(fig, update_plot, frames=timesteps, interval=50, repeat=False)

# Save the animation as a GIF using the pillow writer (faster)
ani.save('emotional_evolution.gif', writer='pillow', fps=15)

plt.show()

"""This Python code simulates the dynamic evolution of an emotional state over time, visualized as a 3D trajectory.  Here's a breakdown:

**1. Simulation Setup:**

* **Parameters:** Defines key parameters like the number of time steps (`timesteps`), the dimensionality of the emotional state (Ego, Attention, Shame; `dim`), and a sensitivity constant (`q`).
* **External and Internal Influences:** Introduces `E_field` (external electric field) to model external factors like therapy or mindfulness, and `B_field` (internal magnetic field) for internal factors such as neuroplasticity. These are represented as random matrices, meaning the effects are variable.
* **Initial State:** Sets initial emotional states (represented by `position` and `velocity`) and synaptic weights (`synaptic_weights`) to simulate the starting point of the system.
* **Trajectory Storage:**  Initializes lists to store the trajectory of the emotional states (`state_trajectory`), healing effects (`healing_trajectory`), shame reduction effects (`shame_trajectory`), and reinforcement learning rewards (`reward_trajectory`).

**2. Influence Functions:**

Defines several functions that represent different factors influencing the emotional state:

* **`disorder_effect`, `healing_effect`, `shame_reduction`:** Model the effects of stress, healing, and shame reduction respectively.  These functions return random vectors, suggesting a stochastic nature to these influences.

* **`adaptive_healing`, `memory_reconsolidation`, `resilience_building`:** Introduce time-dependent effects on the emotional state. These functions seem to model complex, evolving aspects of emotional regulation.

* **`fire_together_wire_together`:** Simulates Hebbian learning, where the co-activation of emotional states strengthens their connections (synaptic weights).

* **`add_noise`:** Incorporates random noise into the simulation to capture the inherent stochasticity of emotional processes.

* **`reinforcement_learning_feedback`:** Implements a simplified reinforcement learning component, providing feedback to the emotional state.

**3. Simulation Loop:**

* Iterates through each time step (`t`).
* **Applying Influences:** At each step, it applies the various influence functions (defined above) to the current emotional state.  These include disorder, healing, shame reduction, time-dependent influences, Hebbian learning, external field effects, internal field effects, and reinforcement learning feedback.
* **State Update:** Updates the emotional state (`position`) and its velocity (`velocity`) based on the cumulative effects of these influences and noise.  The emotional state is normalized to prevent it from diverging too much.
* **Storing Data:** Stores the updated emotional state in `state_trajectory` and tracks specific influences in the other trajectories.

**4. Visualization (Animation):**

* **Normalization:** Uses `MinMaxScaler` to normalize the trajectory data before plotting. This helps scale the data appropriately for visualization.
* **3D Plot:** Creates a 3D plot using `matplotlib`'s `FuncAnimation`, displaying the trajectory of the emotional state (Ego, Attention, Shame).  Each point on the trajectory is highlighted with color.
* **Animation:** The `FuncAnimation` updates the plot frame-by-frame to visualize the evolution of the emotional state over time.
* **Saving Animation:** Saves the animation as an animated GIF (`emotional_evolution.gif`) using the `pillow` writer, providing a visual representation of the system's dynamics.

**In Summary:** This code simulates an abstract model of emotional dynamics. The model uses a combination of predefined functions, random processes, and time-dependent factors to represent the complexities of emotional change.  The visualization provides a dynamic 3D representation of the simulated emotional states over time.  It seems intended as an exploration of conceptual emotional dynamics, rather than a clinically accurate model of human emotions.
"""

import numpy as np
import plotly.graph_objs as go
from sklearn.preprocessing import MinMaxScaler

# Parameters for the simulation
timesteps = 100
dim = 3
q = 1.0

# External electric field
E_field = np.random.uniform(0.1, 0.3, (dim, timesteps))
B_field = np.random.uniform(0.1, 0.2, (dim, dim))

# Initial state
position = np.zeros(dim)
velocity = np.zeros(dim)
synaptic_weights = np.random.uniform(0, 0.1, (dim, dim))

state_trajectory = [position.copy()]

# Function Definitions
def disorder_effect(position):
    return np.random.uniform(-0.1, -0.05, 3)

def healing_effect(position):
    return np.random.uniform(0.05, 0.1, 3)

def shame_reduction(position):
    return np.random.uniform(-0.05, -0.1, 3)

def adaptive_healing(position, t):
    return np.random.uniform(0.05, 0.1, 3) * np.sin(t / 10.0)

def memory_reconsolidation(position, t):
    return np.random.uniform(-0.05, 0.05, 3) * np.cos(t / 20.0)

def resilience_building(position, t):
    return np.random.uniform(0.02, 0.05, 3) * np.log(t + 1)

def fire_together_wire_together(position, previous_state, t):
    return synaptic_weights @ (position - previous_state)

def add_noise(position):
    return position + np.random.uniform(-0.01, 0.01, 3)

def reinforcement_learning_feedback(position):
    return np.random.uniform(-0.05, 0.05, 3)

# Main loop
for t in range(timesteps - 1):
    H_disorder = disorder_effect(position)
    H_healing = healing_effect(position)
    H_shame_reduction = shame_reduction(position)
    H_adaptive_healing = adaptive_healing(position, t)
    H_memory_reconsolidation = memory_reconsolidation(position, t)
    H_resilience = resilience_building(position, t)
    H_connection = fire_together_wire_together(position, state_trajectory[-1], t)
    electric_influence = q * np.dot(E_field[:, t], position)
    magnetic_influence = q * np.cross(velocity, np.dot(B_field, position))

    position += (H_disorder + H_healing + H_shame_reduction + H_adaptive_healing +
                 H_memory_reconsolidation + H_resilience + H_connection +
                 electric_influence + magnetic_influence)

    position = add_noise(position)
    if np.linalg.norm(position) != 0:
        position /= np.linalg.norm(position)

    velocity = position - state_trajectory[-1]
    state_trajectory.append(position.copy())

state_trajectory = np.array(state_trajectory)
scaler = MinMaxScaler()
state_trajectory_scaled = scaler.fit_transform(state_trajectory)

# Create Plotly animation
frames = []
for i in range(len(state_trajectory_scaled)):
    frame = go.Frame(
        data=[
            go.Scatter3d(
                x=state_trajectory_scaled[:i+1, 0],
                y=state_trajectory_scaled[:i+1, 1],
                z=state_trajectory_scaled[:i+1, 2],
                mode="lines+markers",
                line=dict(color="blue"),
                marker=dict(size=4, color="red"),
            )
        ],
        name=f"t={i}"
    )
    frames.append(frame)

layout = go.Layout(
    scene=dict(
        xaxis=dict(title="Ego", range=[0, 1]),
        yaxis=dict(title="Attention", range=[0, 1]),
        zaxis=dict(title="Shame", range=[0, 1]),
    ),
    margin=dict(l=0, r=0, b=0, t=0),
    updatemenus=[
        dict(
            type="buttons",
            showactive=False,
            buttons=[
                dict(label="Play", method="animate", args=[None, dict(frame=dict(duration=50, redraw=True), fromcurrent=True)]),
                dict(label="Pause", method="animate", args=[[None], dict(frame=dict(duration=0, redraw=False), mode="immediate")]),
            ],
        )
    ],
)

fig = go.Figure(
    data=[
        go.Scatter3d(
            x=state_trajectory_scaled[:, 0],
            y=state_trajectory_scaled[:, 1],
            z=state_trajectory_scaled[:, 2],
            mode="lines+markers",
            line=dict(color="blue"),
            marker=dict(size=4, color="red"),
        )
    ],
    layout=layout,
    frames=frames,
)

fig.show()

"""The code simulates the evolution of an emotional state over time, visualized as a 3D trajectory using Plotly.  Let's break down the key aspects:

**1. Simulation Setup:**

* **Parameters:** Sets simulation parameters like `timesteps` (duration), `dim` (emotional state dimensions: Ego, Attention, Shame), and `q` (sensitivity to external/internal fields).

* **External/Internal Fields:**  `E_field` represents external factors (therapy, mindfulness), and `B_field` represents internal factors (neuroplasticity).  These are random matrices, simulating variability in their influence.

* **Initial State:** Sets starting emotional state (`position`, `velocity`), and initial connection strengths (`synaptic_weights`).

* **Trajectory Storage:**  Creates a `state_trajectory` list to store the emotional state at each timestep.

**2. Influence Functions:**

* **Various Effect Functions:**  `disorder_effect`, `healing_effect`, `shame_reduction`, etc., model different influences on the emotional state.  These influences are stochastic, using `np.random.uniform`.

* **Time-Dependent Effects:** Functions like `adaptive_healing`, `memory_reconsolidation`, and `resilience_building` include time (`t`) as a parameter, modeling how these influences may change over time.

* **Hebbian Learning:** `fire_together_wire_together` simulates connections between emotional states strengthening based on co-activation.

* **Noise and Feedback:**  `add_noise` introduces randomness, and `reinforcement_learning_feedback` provides simulated feedback to the system.

**3. Simulation Loop:**

* Iterates through time (`timesteps`).
* **Apply Influences:** Combines the effects of all the influence functions, external fields, and internal fields at each time step.
* **State Update:** Updates `position` and `velocity` based on the combined influences and noise.
* **Normalization:** Normalizes `position` to prevent it from diverging too much, keeping it within a bounded range.
* **Trajectory Storage:** Stores the updated `position` in `state_trajectory`.

**4. Visualization (Plotly Animation):**

* **Normalization:**  Normalizes the trajectory for plotting using `MinMaxScaler`, mapping the data to [0, 1] for consistent visualization.

* **Plotly Animation:**  Generates an animated 3D scatter plot using Plotly.

    * **Frames:** Creates a series of `frames`, where each frame is a snapshot of the trajectory up to a particular time point.

    * **Scatter3d:** Each frame plots the trajectory using `go.Scatter3d`, showing a line connecting the emotional state points and a marker for the current state.

    * **Layout:** Specifies axes labels and ranges for the plot.

    * **Animation Controls:** Includes play and pause buttons to control the animation.

* **Visualization:** The code then uses `fig.show()` to display the interactive Plotly animation, which allows viewers to play/pause the simulation and see the evolution of the simulated emotional state over time.

**In Summary:** This improved code provides an interactive visualization of the simulated emotional state evolution, providing a much better view of the system's dynamics compared to the static GIF in the previous example. The Plotly animation is more flexible and allows for easier examination of the trajectory over time.
"""

# Step 1: Install Required Libraries
!pip install plotly

# Step 2: Import Libraries
import numpy as np
import plotly.graph_objs as go
from sklearn.preprocessing import MinMaxScaler

# Step 3: Define Simulation Parameters
timesteps = 200  # Number of time steps
q = 1  # Arbitrary charge for the particle
position = np.random.rand(3)  # Initial position in 3D space
velocity = np.zeros(3)  # Initial velocity
state_trajectory = [position.copy()]  # Store trajectory for visualization

# External fields
E_field = np.random.randn(3, timesteps) * 0.1  # Electric field
B_field = np.random.randn(3, 3) * 0.05  # Magnetic field matrix

# Define Psychological Effects
def disorder_effect(position):
    return -0.1 * position

def healing_effect(position):
    return 0.05 * (1 - position)

def shame_reduction(position):
    return -0.03 * position

def adaptive_healing(position, t):
    return 0.02 * np.sin(2 * np.pi * t / timesteps)

def memory_reconsolidation(position, t):
    return 0.01 * np.cos(2 * np.pi * t / timesteps)

def resilience_building(position, t):
    return 0.02 * np.exp(-t / timesteps) * (1 - position)

def fire_together_wire_together(position, previous_position, t):
    return 0.01 * (position - previous_position) * (1 - t / timesteps)

def add_noise(position):
    return position + np.random.normal(0, 0.01, position.shape)

# Step 4: Main Simulation Loop
for t in range(timesteps - 1):
    # Combine psychological effects and external forces
    H_disorder = disorder_effect(position)
    H_healing = healing_effect(position)
    H_shame_reduction = shame_reduction(position)
    H_adaptive_healing = adaptive_healing(position, t)
    H_memory_reconsolidation = memory_reconsolidation(position, t)
    H_resilience = resilience_building(position, t)
    H_connection = fire_together_wire_together(position, state_trajectory[-1], t)

    # External field influences
    electric_influence = q * np.dot(E_field[:, t], position)
    magnetic_influence = q * np.cross(velocity, np.dot(B_field, position))

    # Update position and add noise
    position += (H_disorder + H_healing + H_shame_reduction + H_adaptive_healing +
                 H_memory_reconsolidation + H_resilience + H_connection +
                 electric_influence + magnetic_influence)

    position = add_noise(position)

    # Normalize position to keep values bounded
    if np.linalg.norm(position) != 0:
        position /= np.linalg.norm(position)

    # Update velocity and record state
    velocity = position - state_trajectory[-1]
    state_trajectory.append(position.copy())

# Step 5: Normalize Trajectory for Visualization
state_trajectory = np.array(state_trajectory)
scaler = MinMaxScaler()
state_trajectory_scaled = scaler.fit_transform(state_trajectory)

# Step 6: Create 3D Animation with Plotly
frames = []
for i in range(len(state_trajectory_scaled)):
    frame = go.Frame(
        data=[
            go.Scatter3d(
                x=state_trajectory_scaled[:i+1, 0],
                y=state_trajectory_scaled[:i+1, 1],
                z=state_trajectory_scaled[:i+1, 2],
                mode="lines+markers",
                line=dict(color="blue"),
                marker=dict(size=4, color="red"),
            )
        ],
        name=f"t={i}"
    )
    frames.append(frame)

layout = go.Layout(
    scene=dict(
        xaxis=dict(title="Ego", range=[0, 1]),
        yaxis=dict(title="Attention", range=[0, 1]),
        zaxis=dict(title="Shame", range=[0, 1]),
    ),
    margin=dict(l=0, r=0, b=0, t=0),
    updatemenus=[
        dict(
            type="buttons",
            showactive=False,
            buttons=[
                dict(label="Play", method="animate", args=[None, dict(frame=dict(duration=50, redraw=True), fromcurrent=True)]),
                dict(label="Pause", method="animate", args=[[None], dict(frame=dict(duration=0, redraw=False), mode="immediate")]),
            ],
        )
    ],
)

fig = go.Figure(
    data=[
        go.Scatter3d(
            x=state_trajectory_scaled[:, 0],
            y=state_trajectory_scaled[:, 1],
            z=state_trajectory_scaled[:, 2],
            mode="lines+markers",
            line=dict(color="blue"),
            marker=dict(size=4, color="red"),
        )
    ],
    layout=layout,
    frames=frames,
)

fig.show()

"""![Screenshot 2024-12-05 041314.png](data:image/png;base64,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)"""